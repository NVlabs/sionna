<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>sionna.fec.ldpc.decoding &mdash; Sionna 0.11.0 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/sionna.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> Sionna
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../installation.html">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../installation.html#installation-using-pip">Installation using pip</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../installation.html#docker-based-installation">Docker-based Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../installation.html#installation-from-source">Installation from source</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Hello_World.html">“Hello, world!”</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/Discover_Sionna.html">Discover Sionna</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Load-Required-Packages">Load Required Packages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Sionna-Data-flow-and-Design-Paradigms">Sionna Data-flow and Design Paradigms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Let’s-Get-Started---The-First-Layers-(Eager-Mode)">Let’s Get Started - The First Layers (<em>Eager Mode</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Batches-and-Multi-dimensional-Tensors">Batches and Multi-dimensional Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#First-Link-level-Simulation">First Link-level Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Setting-up-the-End-to-end-Model">Setting up the End-to-end Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Run-some-Throughput-Tests-(Graph-Mode)">Run some Throughput Tests (Graph Mode)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Bit-Error-Rate-(BER)-Monte-Carlo-Simulations">Bit-Error Rate (BER) Monte-Carlo Simulations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../examples/Discover_Sionna.html#Conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials.html">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials.html#for-beginners">For Beginners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html">Part 1: Getting Started with Sionna</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Imports-&amp;-Basics">Imports &amp; Basics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Sionna-Data-flow-and-Design-Paradigms">Sionna Data-flow and Design Paradigms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Hello,-Sionna!">Hello, Sionna!</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Communication-Systems-as-Keras-Models">Communication Systems as Keras Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Forward-Error-Correction-(FEC)">Forward Error Correction (FEC)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Eager-vs-Graph-Mode">Eager vs Graph Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part1.html#Exercise">Exercise</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part2.html">Part 2: Differentiable Communication Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part2.html#Imports">Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part2.html#Gradient-Computation-Through-End-to-end-Systems">Gradient Computation Through End-to-end Systems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part2.html#Creating-Custom-Layers">Creating Custom Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part2.html#Setting-up-Training-Loops">Setting up Training Loops</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html">Part 3: Advanced Link-level Simulations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#Imports">Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#OFDM-Resource-Grid-and-Stream-Management">OFDM Resource Grid and Stream Management</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#Stream-Management">Stream Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#OFDM-Resource-Grid">OFDM Resource Grid</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#Antenna-Arrays">Antenna Arrays</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#Channel-Model">Channel Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part3.html#Uplink-Transmission-in-the-Frequency-Domain">Uplink Transmission in the Frequency Domain</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html">Part 4: Toward Learned Receivers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#Imports">Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#Simulation-Parameters">Simulation Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#Implemention-of-an-Advanced-Neural-Receiver">Implemention of an Advanced Neural Receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#Training-the-Neural-Receiver">Training the Neural Receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#Benchmarking-the-Neural-Receiver">Benchmarking the Neural Receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#Conclusion">Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Sionna_tutorial_part4.html#References">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html">Basic MIMO Simulations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html#Table-of-Contents">Table of Contents</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html#Simple-uncoded-transmission">Simple uncoded transmission</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html#Adding-spatial-correlation">Adding spatial correlation</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html#Extension-to-channel-coding">Extension to channel coding</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Simple_MIMO_Simulation.html#BER-simulations-using-a-Keras-model">BER simulations using a Keras model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html">Pulse-shaping Basics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html#Pulse-shaping-of-a-sequence-of-QAM-symbols">Pulse-shaping of a sequence of QAM symbols</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html#Recovering-the-QAM-symbols-through-matched-filtering-and-downsampling">Recovering the QAM symbols through matched filtering and downsampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html#Investigating-the-ACLR">Investigating the ACLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Pulse_shaping_basics.html#Windowing">Windowing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html">Optical Channel with Lumped Amplification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Setup">Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Impulse-Generation">Impulse Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Attenuation">Attenuation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Amplified-Spontaneous-Emission-Noise">Amplified Spontaneous Emission Noise</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Channel-Configuration">Channel Configuration</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Transmission">Transmission</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Chromatic-Dispersion">Chromatic Dispersion</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#id1">Channel Configuration</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#id2">Transmission</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Kerr-Nonlinearity">Kerr Nonlinearity</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Channel-configuration">Channel configuration</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#id4">Transmission</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#Split-Step-Fourier-Method">Split-Step Fourier Method</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#id5">Channel Configuration</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#id6">Transmission</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Optical_Lumped_Amplification_Channel.html#References">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tutorials.html#for-experts">For Experts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html">5G Channel Coding and Rate-Matching: Polar vs. LDPC Codes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#BER-Performance-of-5G-Coding-Schemes">BER Performance of 5G Coding Schemes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#A-Deeper-Look-into-the-Polar-Code-Module">A Deeper Look into the Polar Code Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#Rate-Matching-and-Rate-Recovery">Rate-Matching and Rate-Recovery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#Throughput-and-Decoding-Complexity">Throughput and Decoding Complexity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/5G_Channel_Coding_Polar_vs_LDPC_Codes.html#References">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html">Bit-Interleaved Coded Modulation (BICM)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#System-Block-Diagram">System Block Diagram</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#A-Simple-BICM-System">A Simple BICM System</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#Constellations-and-Bit-Channels">Constellations and Bit-Channels</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#Simple-BER-Simulations">Simple BER Simulations</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#All-zero-Codeword-Simulations">All-zero Codeword Simulations</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#Remove-Encoder:-Simulate-QPSK-with-All-zero-Codeword-Transmission">Remove Encoder: Simulate QPSK with All-zero Codeword Transmission</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#Remove-(De-)Mapping:-Approximate-the-LLR-Distribution-of-the-All-zero-Codeword-(and-BPSK/QPSK)">Remove (De-)Mapping: Approximate the LLR Distribution of the All-zero Codeword (and BPSK/QPSK)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#The-Role-of-the-Scrambler">The Role of the Scrambler</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#EXIT-Charts">EXIT Charts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#Mismatched-Demapping-and-the-Advantages-of-Min-sum-Decoding">Mismatched Demapping and the Advantages of Min-sum Decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Bit_Interleaved_Coded_Modulation.html#References">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html">MIMO OFDM Transmissions over the CDL Channel Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Table-of-Contents">Table of Contents</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#System-Setup">System Setup</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Stream-Management">Stream Management</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#OFDM-Resource-Grid-&amp;-Pilot-Pattern">OFDM Resource Grid &amp; Pilot Pattern</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Antenna-Arrays">Antenna Arrays</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#CDL-Channel-Model">CDL Channel Model</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Other-Physical-Layer-Components">Other Physical Layer Components</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Simulations">Simulations</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Uplink-Transmission-in-the-Frequency-Domain">Uplink Transmission in the Frequency Domain</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Uplink-Transmission-in-the-Time-Domain">Uplink Transmission in the Time Domain</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Downlink-Transmission-in-the-Frequency-Domain">Downlink Transmission in the Frequency Domain</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Understand-the-Difference-Between-the-CDL-Models">Understand the Difference Between the CDL Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Create-an-End-to-End-Keras-Model">Create an End-to-End Keras Model</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Compare-Uplink-Performance-Over-the-Different-CDL-Models">Compare Uplink Performance Over the Different CDL Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Compare-Downlink-Performance-Over-the-Different-CDL-Models">Compare Downlink Performance Over the Different CDL Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Evaluate-the-Impact-of-Mobility">Evaluate the Impact of Mobility</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/MIMO_OFDM_Transmissions_over_CDL.html#Evaluate-the-Impact-of-Insufficient-Cyclic-Prefix-Length">Evaluate the Impact of Insufficient Cyclic Prefix Length</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Neural_Receiver.html">Neural Receiver for OFDM SIMO Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#Simulation-Parameters">Simulation Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#Neural-Receiver">Neural Receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#End-to-end-System">End-to-end System</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#End-to-end-System-as-a-Keras-Model">End-to-end System as a Keras Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#Evaluation-of-the-Baselines">Evaluation of the Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#Training-the-Neural-Receiver">Training the Neural Receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#Evaluation-of-the-Neural-Receiver">Evaluation of the Neural Receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#Pre-computed-Results">Pre-computed Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Neural_Receiver.html#References">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html">Realistic Multiuser MIMO OFDM Simulations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#System-Setup">System Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#Uplink-Transmissions-in-the-Frequency-Domain">Uplink Transmissions in the Frequency Domain</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#Compare-Estimated-and-Actual-Frequency-Responses">Compare Estimated and Actual Frequency Responses</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#Understand-the-Difference-Between-the-Channel-Models">Understand the Difference Between the Channel Models</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Realistic_Multiuser_MIMO_Simulations.html#Setup-a-Keras-Model-for-BER-simulations">Setup a Keras Model for BER simulations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Autoencoder.html">End-to-end Learning with Autoencoders</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#Simulation-Parameters">Simulation Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#Neural-Demapper">Neural Demapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#Trainable-End-to-end-System:-Conventional-Training">Trainable End-to-end System: Conventional Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#Trainable-End-to-end-System:-RL-based-Training">Trainable End-to-end System: RL-based Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#Evaluation">Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#Visualizing-the-Learned-Constellations">Visualizing the Learned Constellations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Autoencoder.html#References">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html">Weighted Belief Propagation Decoding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Weighted-BP-for-BCH-Codes">Weighted BP for BCH Codes</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Weights-before-Training-and-Simulation-of-BER">Weights <em>before</em> Training and Simulation of BER</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Training">Training</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Results">Results</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Further-Experiments">Further Experiments</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Damped-BP">Damped BP</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#Learning-the-5G-LDPC-Code">Learning the 5G LDPC Code</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/Weighted_BP_Algorithm.html#References">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/CIR_Dataset.html">Channel Models from Datasets</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/CIR_Dataset.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/CIR_Dataset.html#Simulation-Parameters">Simulation Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/CIR_Dataset.html#Creating-a-Simple-Dataset">Creating a Simple Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/CIR_Dataset.html#Generators">Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/CIR_Dataset.html#Use-the-Channel-Model-for-OFDM-Transmissions">Use the Channel Model for OFDM Transmissions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../examples/DeepMIMO.html">Using the DeepMIMO Dataset with Sionna</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/DeepMIMO.html#Table-of-Contents">Table of Contents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/DeepMIMO.html#GPU-Configuration-and-Imports">GPU Configuration and Imports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/DeepMIMO.html#Configuration-of-DeepMIMO">Configuration of DeepMIMO</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/DeepMIMO.html#Visualization-of-the-dataset">Visualization of the dataset</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/DeepMIMO.html#Using-DeepMIMO-with-Sionna">Using DeepMIMO with Sionna</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../examples/DeepMIMO.html#Random-Sampling-of-Multi-User-Channels">Random Sampling of Multi-User Channels</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/DeepMIMO.html#Link-level-Simulations-using-Sionna-and-DeepMIMO">Link-level Simulations using Sionna and DeepMIMO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../examples/DeepMIMO.html#DeepMIMO-License-and-Citation">DeepMIMO License and Citation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../made_with_sionna.html">“Made with Sionna”</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../made_with_sionna.html#list-of-projects">List of Projects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../made_with_sionna.html#gnns-for-channel-decoding">GNNs for Channel Decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../made_with_sionna.html#dl-based-synchronization-of-nb-iot">DL-based Synchronization of NB-IoT</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/sionna.html">API Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/fec.html">Forward Error Correction (FEC)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.ldpc.html">Low-Density Parity-Check (LDPC)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.ldpc.html#encoder">Encoder</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.ldpc.html#ldpc5gencoder">LDPC5GEncoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.ldpc.html#allzeroencoder">AllZeroEncoder</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.ldpc.html#decoder">Decoder</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.ldpc.html#ldpcbpdecoder">LDPCBPDecoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.ldpc.html#ldpc5gdecoder">LDPC5GDecoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.polar.html">Polar Codes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.polar.html#polar-encoding">Polar Encoding</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#polar5gencoder">Polar5GEncoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#polarencoder">PolarEncoder</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.polar.html#polar-decoding">Polar Decoding</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#polar5gdecoder">Polar5GDecoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#polarscdecoder">PolarSCDecoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#polarscldecoder">PolarSCLDecoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#polarbpdecoder">PolarBPDecoder</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.polar.html#polar-utility-functions">Polar Utility Functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#generate-5g-ranking">generate_5g_ranking</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#generate-polar-transform-mat">generate_polar_transform_mat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#generate-rm-code">generate_rm_code</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.polar.html#generate-dense-polar">generate_dense_polar</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.conv.html">Convolutional Codes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.conv.html#convolutional-encoding">Convolutional Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.conv.html#viterbi-decoding">Viterbi Decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.conv.html#bcjr-decoding">BCJR Decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.conv.html#convolutional-code-utility-functions">Convolutional Code Utility Functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.conv.html#trellis">Trellis</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.conv.html#polynomial-selector">polynomial_selector</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.turbo.html">Turbo Codes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.turbo.html#turbo-encoding">Turbo Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.turbo.html#turbo-decoding">Turbo Decoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.turbo.html#turbo-utility-functions">Turbo Utility Functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.turbo.html#turbotermination">TurboTermination</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.turbo.html#polynomial-selector">polynomial_selector</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.turbo.html#puncture-pattern">puncture_pattern</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.crc.html">Cyclic Redundancy Check (CRC)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.crc.html#crcencoder">CRCEncoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.crc.html#crcdecoder">CRCDecoder</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.interleaving.html">Interleaving</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.interleaving.html#interleaver">Interleaver</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.interleaving.html#rowcolumninterleaver">RowColumnInterleaver</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.interleaving.html#randominterleaver">RandomInterleaver</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.interleaving.html#turbo3gppinterleaver">Turbo3GPPInterleaver</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.interleaving.html#deinterleaver">Deinterleaver</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.scrambling.html">Scrambling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.scrambling.html#scrambler">Scrambler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.scrambling.html#descrambler">Descrambler</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/fec.utils.html">Utility Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.utils.html#binary-linear-codes">(Binary) Linear Codes</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#linearencoder">LinearEncoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#load-parity-check-examples">load_parity_check_examples</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#alist2mat">alist2mat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#load-alist">load_alist</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#generate-reg-ldpc">generate_reg_ldpc</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#make-systematic">make_systematic</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#gm2pcm">gm2pcm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#pcm2gm">pcm2gm</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#verify-gm-pcm">verify_gm_pcm</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.utils.html#exit-analysis">EXIT Analysis</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#plot-exit-chart">plot_exit_chart</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#get-exit-analytic">get_exit_analytic</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#plot-trajectory">plot_trajectory</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/fec.utils.html#miscellaneous">Miscellaneous</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#gaussianpriorsource">GaussianPriorSource</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#generate-prng-seq">generate_prng_seq</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#bin2int">bin2int</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#int2bin">int2bin</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#bin2int-tf">bin2int_tf</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#int2bin-tf">int2bin_tf</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#llr2mi">llr2mi</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#j-fun">j_fun</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#j-fun-inv">j_fun_inv</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#j-fun-tf">j_fun_tf</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/fec.utils.html#j-fun-inv-tf">j_fun_inv_tf</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/mapping.html">Mapping</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mapping.html#constellations">Constellations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#constellation">Constellation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#qam">qam</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#pam">pam</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#pam-gray">pam_gray</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mapping.html#mapper">Mapper</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mapping.html#demapping">Demapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#demapper">Demapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#demapperwithprior">DemapperWithPrior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#symboldemapper">SymbolDemapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#symboldemapperwithprior">SymbolDemapperWithPrior</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mapping.html#utility-functions">Utility Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#symbollogits2llrs">SymbolLogits2LLRs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#llrs2symbollogits">LLRs2SymbolLogits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#symbollogits2llrswithprior">SymbolLogits2LLRsWithPrior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mapping.html#symbollogits2moments">SymbolLogits2Moments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/channel.html">Channel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/channel.wireless.html">Wireless</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#awgn">AWGN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#flat-fading-channel">Flat-fading channel</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#flatfadingchannel">FlatFadingChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#generateflatfadingchannel">GenerateFlatFadingChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#applyflatfadingchannel">ApplyFlatFadingChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#spatialcorrelation">SpatialCorrelation</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#kroneckermodel">KroneckerModel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#percolumnmodel">PerColumnModel</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#channel-model-interface">Channel model interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#time-domain-channel">Time domain channel</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#timechannel">TimeChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#generatetimechannel">GenerateTimeChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#applytimechannel">ApplyTimeChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#cir-to-time-channel">cir_to_time_channel</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#channel-with-ofdm-waveform">Channel with OFDM waveform</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#ofdmchannel">OFDMChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#generateofdmchannel">GenerateOFDMChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#applyofdmchannel">ApplyOFDMChannel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#cir-to-ofdm-channel">cir_to_ofdm_channel</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#rayleigh-block-fading">Rayleigh block fading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#gpp-38-901-channel-models">3GPP 38.901 channel models</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#panelarray">PanelArray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#antenna">Antenna</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#antennaarray">AntennaArray</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#tapped-delay-line-tdl">Tapped delay line (TDL)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#clustered-delay-line-cdl">Clustered delay line (CDL)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#urban-microcell-umi">Urban microcell (UMi)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#urban-macrocell-uma">Urban macrocell (UMa)</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#rural-macrocell-rma">Rural macrocell (RMa)</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#external-datasets">External datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.wireless.html#utility-functions">Utility functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#subcarrier-frequencies">subcarrier_frequencies</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#time-lag-discrete-time-channel">time_lag_discrete_time_channel</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#deg-2-rad">deg_2_rad</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#rad-2-deg">rad_2_deg</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#wrap-angle-0-360">wrap_angle_0_360</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#drop-uts-in-sector">drop_uts_in_sector</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#relocate-uts">relocate_uts</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#set-3gpp-scenario-parameters">set_3gpp_scenario_parameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#gen-single-sector-topology">gen_single_sector_topology</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#gen-single-sector-topology-interferers">gen_single_sector_topology_interferers</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#exp-corr-mat">exp_corr_mat</a></li>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.wireless.html#one-ring-corr-mat">one_ring_corr_mat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/channel.optical.html">Optical</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.optical.html#split-step-fourier-method">Split-step Fourier method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.optical.html#erbium-doped-fiber-amplifier">Erbium-doped fiber amplifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/channel.optical.html#utility-functions">Utility functions</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../../../../api/channel.optical.html#time-frequency-vector">time_frequency_vector</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/ofdm.html">Orthogonal Frequency-Division Multiplexing (OFDM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#resource-grid">Resource Grid</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#resourcegrid">ResourceGrid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#resourcegridmapper">ResourceGridMapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#resourcegriddemapper">ResourceGridDemapper</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#removenulledsubcarriers">RemoveNulledSubcarriers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#modulation-demodulation">Modulation &amp; Demodulation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#ofdmmodulator">OFDMModulator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#ofdmdemodulator">OFDMDemodulator</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#pilot-pattern">Pilot Pattern</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#pilotpattern">PilotPattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#emptypilotpattern">EmptyPilotPattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#kroneckerpilotpattern">KroneckerPilotPattern</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#channel-estimation">Channel Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#lschannelestimator">LSChannelEstimator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#nearestneighborinterpolator">NearestNeighborInterpolator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#linearinterpolator">LinearInterpolator</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#precoding">Precoding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#zfprecoder">ZFPrecoder</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#equalization">Equalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#lmmseequalizer">LMMSEEqualizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/ofdm.html#detection">Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#maximumlikelihooddetector">MaximumLikelihoodDetector</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/ofdm.html#maximumlikelihooddetectorwithprior">MaximumLikelihoodDetectorWithPrior</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/mimo.html">Multiple-Input Multiple-Output (MIMO)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mimo.html#stream-management">Stream Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mimo.html#precoding">Precoding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#zero-forcing-precoder">zero_forcing_precoder</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mimo.html#equalization">Equalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#lmmse-equalizer">lmmse_equalizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#zf-equalizer">zf_equalizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#mf-equalizer">mf_equalizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mimo.html#detection">Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#maximumlikelihooddetector">MaximumLikelihoodDetector</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#maximumlikelihooddetectorwithprior">MaximumLikelihoodDetectorWithPrior</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/mimo.html#utility-functions">Utility Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#complex2real-vector">complex2real_vector</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#real2complex-vector">real2complex_vector</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#complex2real-matrix">complex2real_matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#real2complex-matrix">real2complex_matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#complex2real-covariance">complex2real_covariance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#real2complex-covariance">real2complex_covariance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#complex2real-channel">complex2real_channel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#real2complex-channel">real2complex_channel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/mimo.html#whiten-channel">whiten_channel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/signal.html">Signal</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/signal.html#filters">Filters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#sincfilter">SincFilter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#raisedcosinefilter">RaisedCosineFilter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#rootraisedcosinefilter">RootRaisedCosineFilter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#customfilter">CustomFilter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#id1">Filter</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/signal.html#window-functions">Window functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#hannwindow">HannWindow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#hammingwindow">HammingWindow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#blackmanwindow">BlackmanWindow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#customwindow">CustomWindow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#id2">Window</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/signal.html#utility-functions">Utility Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#convolve">convolve</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#fft">fft</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#ifft">ifft</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#upsampling">Upsampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#downsampling">Downsampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#empirical-psd">empirical_psd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/signal.html#empirical-aclr">empirical_aclr</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/utils.html">Utility Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/utils.html#metrics">Metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#biterrorrate">BitErrorRate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#bitwisemutualinformation">BitwiseMutualInformation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#compute-ber">compute_ber</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#compute-bler">compute_bler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#compute-ser">compute_ser</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#count-errors">count_errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#count-block-errors">count_block_errors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/utils.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#expand-to-rank">expand_to_rank</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#flatten-dims">flatten_dims</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#flatten-last-dims">flatten_last_dims</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#insert-dims">insert_dims</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#split-dims">split_dims</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#matrix-sqrt">matrix_sqrt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#matrix-sqrt-inv">matrix_sqrt_inv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#matrix-inv">matrix_inv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#matrix-pinv">matrix_pinv</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../api/utils.html#miscellaneous">Miscellaneous</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#binarysource">BinarySource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#symbolsource">SymbolSource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#qamsource">QAMSource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#pamsource">PAMSource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#plotber">PlotBER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#sim-ber">sim_ber</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#ebnodb2no">ebnodb2no</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#hard-decisions">hard_decisions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#plot-ber">plot_ber</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#complex-normal">complex_normal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#log2">log2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../api/utils.html#log10">log10</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../api/config.html">Configuration</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVlabs/sionna/discussions">Discussions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVlabs/sionna/issues">Report an Issue</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Sionna</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>sionna.fec.ldpc.decoding</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for sionna.fec.ldpc.decoding</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># SPDX-FileCopyrightText: Copyright (c) 2021-2022 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.</span>
<span class="c1"># SPDX-License-Identifier: Apache-2.0</span>
<span class="c1">#</span>
<span class="sd">&quot;&quot;&quot;Layers for channel decoding and utility functions.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span> <span class="c1"># for sparse H matrix computations</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Layer</span>
<span class="kn">from</span> <span class="nn">sionna.fec.ldpc.encoding</span> <span class="kn">import</span> <span class="n">LDPC5GEncoder</span>
<span class="kn">from</span> <span class="nn">sionna.fec.utils</span> <span class="kn">import</span> <span class="n">llr2mi</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<div class="viewcode-block" id="LDPCBPDecoder"><a class="viewcode-back" href="../../../../api/fec.ldpc.html#sionna.fec.ldpc.decoding.LDPCBPDecoder">[docs]</a><span class="k">class</span> <span class="nc">LDPCBPDecoder</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;LDPCBPDecoder(pcm, trainable=False, cn_type=&#39;boxplus-phi&#39;, hard_out=True, track_exit=False, num_iter=20, stateful=False,output_dtype=tf.float32, **kwargs)</span>

<span class="sd">    Iterative belief propagation decoder for low-density parity-check (LDPC)</span>
<span class="sd">    codes and other `codes on graphs`.</span>

<span class="sd">    This class defines a generic belief propagation decoder for decoding</span>
<span class="sd">    with arbitrary parity-check matrices. It can be used to iteratively</span>
<span class="sd">    estimate/recover the transmitted codeword (or information bits) based on the</span>
<span class="sd">    LLR-values of the received noisy codeword observation.</span>

<span class="sd">    The decoder implements the flooding SPA algorithm [Ryan]_, i.e., all nodes</span>
<span class="sd">    are updated in a parallel fashion. Different check node update functions are</span>
<span class="sd">    available</span>

<span class="sd">    (1) `boxplus`</span>

<span class="sd">        .. math::</span>
<span class="sd">            y_{j \to i} = 2 \operatorname{tanh}^{-1} \left( \prod_{i&#39; \in \mathcal{N}_(j) \setminus i} \operatorname{tanh} \left( \frac{x_{i&#39; \to j}}{2} \right) \right)</span>

<span class="sd">    (2) `boxplus-phi`</span>

<span class="sd">        .. math::</span>
<span class="sd">            y_{j \to i} = \alpha_{j \to i} \cdot \phi \left( \sum_{i&#39; \in \mathcal{N}_(j) \setminus i} \phi \left( |x_{i&#39; \to j}|\right) \right)</span>

<span class="sd">        with :math:`\phi(x)=-\operatorname{log}(\operatorname{tanh} \left(\frac{x}{2}) \right)`</span>

<span class="sd">    (3) `minsum`</span>

<span class="sd">        .. math::</span>
<span class="sd">            \qquad y_{j \to i} = \alpha_{j \to i} \cdot {min}_{i&#39; \in \mathcal{N}_(j) \setminus i} \left(|x_{i&#39; \to j}|\right)</span>

<span class="sd">    where :math:`y_{j \to i}` denotes the message from check node (CN) *j* to</span>
<span class="sd">    variable node (VN) *i* and :math:`x_{i \to j}` from VN *i* to CN *j*,</span>
<span class="sd">    respectively. Further, :math:`\mathcal{N}_(j)` denotes all indices of</span>
<span class="sd">    connected VNs to CN *j* and</span>

<span class="sd">    .. math::</span>
<span class="sd">        \alpha_{j \to i} = \prod_{i&#39; \in \mathcal{N}_(j) \setminus i} \operatorname{sign}(x_{i&#39; \to j})</span>

<span class="sd">    is the sign of the outgoing message. For further details we refer to</span>
<span class="sd">    [Ryan]_.</span>

<span class="sd">    Note that for full 5G 3GPP NR compatibility, the correct puncturing and</span>
<span class="sd">    shortening patterns must be applied (cf. [Richardson]_ for details), this</span>
<span class="sd">    can be done by :class:`~sionna.fec.ldpc.decoding.LDPC5GEncoder` and</span>
<span class="sd">    :class:`~sionna.fec.ldpc.decoding.LDPC5GDecoder`, respectively.</span>

<span class="sd">    If required, the decoder can be made trainable and is fully differentiable</span>
<span class="sd">    by following the concept of `weighted BP` [Nachmani]_ as shown in Fig. 1</span>
<span class="sd">    leading to</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_{j \to i} = 2 \operatorname{tanh}^{-1} \left( \prod_{i&#39; \in \mathcal{N}_(j) \setminus i} \operatorname{tanh} \left( \frac{\textcolor{red}{w_{i&#39; \to j}} \cdot x_{i&#39; \to j}}{2} \right) \right)</span>

<span class="sd">    where :math:`w_{i \to j}` denotes the trainable weight of message :math:`x_{i \to j}`.</span>
<span class="sd">    Please note that the training of some check node types may be not supported.</span>

<span class="sd">    ..  figure:: ../figures/weighted_bp.png</span>

<span class="sd">        Fig. 1: Weighted BP as proposed in [Nachmani]_.</span>


<span class="sd">    The class inherits from the Keras layer class and can be used as layer in a</span>
<span class="sd">    Keras model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">        pcm: ndarray</span>
<span class="sd">            An ndarray of shape `[n-k, n]` defining the parity-check matrix</span>
<span class="sd">            consisting only of `0` or `1` entries. Can be also of type `scipy.</span>
<span class="sd">            sparse.csr_matrix` or `scipy.sparse.csc_matrix`.</span>

<span class="sd">        trainable: bool</span>
<span class="sd">            Defaults to False. If True, every outgoing variable node message is</span>
<span class="sd">            scaled with a trainable scalar.</span>

<span class="sd">        cn_type: str</span>
<span class="sd">            A string defaults to &#39;&quot;boxplus-phi&quot;&#39;. One of</span>
<span class="sd">            {`&quot;boxplus&quot;`, `&quot;boxplus-phi&quot;`, `&quot;minsum&quot;`} where</span>
<span class="sd">            &#39;&quot;boxplus&quot;&#39; implements the single-parity-check APP decoding rule.</span>
<span class="sd">            &#39;&quot;boxplus-phi&quot;&#39; implements the numerical more stable version of</span>
<span class="sd">            boxplus [Ryan]_.</span>
<span class="sd">            &#39;&quot;minsum&quot;&#39; implements the min-approximation of the CN</span>
<span class="sd">            update rule [Ryan]_.</span>

<span class="sd">        hard_out: bool</span>
<span class="sd">            Defaults to True. If True, the decoder provides hard-decided</span>
<span class="sd">            codeword bits instead of soft-values.</span>

<span class="sd">        track_exit: bool</span>
<span class="sd">            Defaults to False. If True, the decoder tracks EXIT</span>
<span class="sd">            characteristics. Note that this requires the all-zero</span>
<span class="sd">            CW as input.</span>

<span class="sd">        num_iter: int</span>
<span class="sd">            Defining the number of decoder iteration (no early stopping used at</span>
<span class="sd">            the moment!).</span>

<span class="sd">        stateful: bool</span>
<span class="sd">            Defaults to False. If True, the internal VN messages ``msg_vn``</span>
<span class="sd">            from the last decoding iteration are returned, and ``msg_vn`` or</span>
<span class="sd">            `None` needs to be given as a second input when calling the decoder.</span>
<span class="sd">            This is required for iterative demapping and decoding.</span>

<span class="sd">        output_dtype: tf.DType</span>
<span class="sd">            Defaults to tf.float32. Defines the output datatype of the layer</span>
<span class="sd">            (internal precision remains tf.float32).</span>

<span class="sd">    Input</span>
<span class="sd">    -----</span>
<span class="sd">    llrs_ch or (llrs_ch, msg_vn):</span>
<span class="sd">        Tensor or Tuple (only required if ``stateful`` is True):</span>

<span class="sd">    llrs_ch: [...,n], tf.float32</span>
<span class="sd">        2+D tensor containing the channel logits/llr values.</span>

<span class="sd">    msg_vn: None or RaggedTensor, tf.float32</span>
<span class="sd">        Ragged tensor of VN messages.</span>
<span class="sd">        Required only if ``stateful`` is True.</span>

<span class="sd">    Output</span>
<span class="sd">    ------</span>
<span class="sd">        : [...,n], tf.float32</span>
<span class="sd">            2+D Tensor of same shape as ``inputs`` containing</span>
<span class="sd">            bit-wise soft-estimates (or hard-decided bit-values) of all</span>
<span class="sd">            codeword bits.</span>

<span class="sd">        : RaggedTensor, tf.float32:</span>
<span class="sd">            Tensor of VN messages.</span>
<span class="sd">            Returned only if ``stateful`` is set to True.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">        pcm: ndarray</span>
<span class="sd">            An ndarray of shape `[n-k, n]` defining the parity-check matrix</span>
<span class="sd">            consisting only of `0` or `1` entries. Can be also of type `scipy.</span>
<span class="sd">            sparse.csr_matrix` or `scipy.sparse.csc_matrix`.</span>

<span class="sd">        num_cns: int</span>
<span class="sd">            Defining the number of check nodes.</span>

<span class="sd">        num_vns: int</span>
<span class="sd">            Defining the number of variable nodes.</span>

<span class="sd">        num_edges: int</span>
<span class="sd">            Defining the total number of edges.</span>

<span class="sd">        trainable: bool</span>
<span class="sd">            If True, the decoder uses trainable weights.</span>

<span class="sd">        _atanh_clip_value: float</span>
<span class="sd">            Defining the internal clipping value before the atanh is applied</span>
<span class="sd">            (relates to the CN update).</span>

<span class="sd">        _cn_type: str</span>
<span class="sd">            Defining the CN update function type.</span>

<span class="sd">        _cn_update:</span>
<span class="sd">            A function defining the CN update.</span>

<span class="sd">        _hard_out: bool</span>
<span class="sd">            If True, the decoder outputs hard-decided bits.</span>

<span class="sd">        _cn_con: ndarray</span>
<span class="sd">            An ndarray of shape `[num_edges]` defining all edges from check</span>
<span class="sd">            node perspective.</span>

<span class="sd">        _vn_con: ndarray</span>
<span class="sd">            An ndarray of shape `[num_edges]` defining all edges from variable</span>
<span class="sd">            node perspective.</span>

<span class="sd">        _vn_mask_tf: tf.float32</span>
<span class="sd">            A ragged Tensor of shape `[num_vns, None]` defining the incoming</span>
<span class="sd">            message indices per VN. The second dimension is ragged and depends</span>
<span class="sd">            on the node degree.</span>

<span class="sd">        _cn_mask_tf: tf.float32</span>
<span class="sd">            A ragged Tensor of shape `[num_cns, None]` defining the incoming</span>
<span class="sd">            message indices per CN. The second dimension is ragged and depends</span>
<span class="sd">            on the node degree.</span>

<span class="sd">        _ind_cn: ndarray</span>
<span class="sd">            An ndarray of shape `[num_edges]` defining the permutation index to</span>
<span class="sd">            rearrange messages from variable into check node perspective.</span>

<span class="sd">        _ind_cn_inv: ndarray</span>
<span class="sd">            An ndarray of shape `[num_edges]` defining the permutation index to</span>
<span class="sd">            rearrange messages from check into variable node perspective.</span>

<span class="sd">        _vn_row_splits: ndarray</span>
<span class="sd">            An ndarray of shape `[num_vns+1]` defining the row split positions</span>
<span class="sd">            of a 1D vector consisting of all edges messages. Used to build a</span>
<span class="sd">            ragged Tensor of incoming VN messages.</span>

<span class="sd">        _cn_row_splits: ndarray</span>
<span class="sd">            An ndarray of shape `[num_cns+1]` defining the row split positions</span>
<span class="sd">            of a 1D vector consisting of all edges messages. Used to build a</span>
<span class="sd">            ragged Tensor of incoming CN messages.</span>

<span class="sd">        _edge_weights: tf.float32</span>
<span class="sd">            A Tensor of shape `[num_edges]` defining a (trainable) weight per</span>
<span class="sd">            outgoing VN message.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError</span>
<span class="sd">            If the shape of ``pcm`` is invalid or contains other values than</span>
<span class="sd">            `0` or `1` or dtype is not `tf.float32`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``trainable`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``track_exit`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``hard_out`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``stateful`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``cn_type`` is not `str`.</span>

<span class="sd">        ValueError</span>
<span class="sd">            If ``num_iter`` is not an integer greater (or equal) `0`.</span>

<span class="sd">        ValueError</span>
<span class="sd">            If ``output_dtype`` is not</span>
<span class="sd">            {tf.float16, tf.float32, tf.float64}.</span>

<span class="sd">        ValueError</span>
<span class="sd">            If ``inputs`` is not of shape `[batch_size, n]`.</span>

<span class="sd">        InvalidArgumentError</span>
<span class="sd">            When rank(``inputs``)&lt;2.</span>
<span class="sd">    Note</span>
<span class="sd">    ----</span>
<span class="sd">        As decoding input logits</span>
<span class="sd">        :math:`\operatorname{log} \frac{p(x=1)}{p(x=0)}` are</span>
<span class="sd">        assumed for compatibility with the learning framework, but internally</span>
<span class="sd">        log-likelihood ratios (LLRs) with definition :math:`\operatorname{log} \frac{p(x=0)}{p(x=1)}` are used.</span>

<span class="sd">        The decoder is not (particularly) optimized for quasi-cyclic (QC) LDPC</span>
<span class="sd">        codes and, thus, supports arbitrary parity-check matrices.</span>

<span class="sd">        The decoder is implemented by using &#39;&quot;ragged Tensors&quot;&#39; [TF_ragged]_ to</span>
<span class="sd">        account for arbitrary node degrees. To avoid a performance degradation</span>
<span class="sd">        caused by a severe indexing overhead, the batch-dimension is shifted to</span>
<span class="sd">        the last dimension during decoding.</span>

<span class="sd">        If the decoder is made trainable [Nachmani]_, for performance</span>
<span class="sd">        improvements only variable to check node messages are scaled as the VN</span>
<span class="sd">        operation is linear and, thus, would not increase the expressive power</span>
<span class="sd">        of the weights.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pcm</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cn_type</span><span class="o">=</span><span class="s1">&#39;boxplus-phi&#39;</span><span class="p">,</span>
                 <span class="n">hard_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">track_exit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">num_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">stateful</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">output_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainable</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;trainable must be bool.&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hard_out</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;hard_out must be bool.&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">track_exit</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;track_exit must be bool.&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cn_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="p">,</span> <span class="s1">&#39;cn_type must be str.&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="s1">&#39;num_iter must be int.&#39;</span>
        <span class="k">assert</span> <span class="n">num_iter</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;num_iter cannot be negative.&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stateful</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;stateful must be bool.&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">DType</span><span class="p">),</span> \
                                <span class="s1">&#39;output_dtype must be tf.Dtype.&#39;</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">pcm</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)),</span> <span class="s1">&#39;PC matrix </span><span class="se">\</span>
<span class="s1">                must be binary.&#39;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">pcm</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">pcm</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)),</span> \
                <span class="s1">&#39;PC matrix must be binary.&#39;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csc_matrix</span><span class="p">):</span>
            <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">pcm</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">pcm</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)),</span> \
                <span class="s1">&#39;PC matrix must be binary.&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Unsupported dtype of pcm.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;output_dtype must be {tf.float16, tf.float32, tf.float64}.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Note: decoder uses tf.float32 for internal calculations.&#39;</span><span class="p">)</span>

        <span class="c1"># init decoder parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pcm</span> <span class="o">=</span> <span class="n">pcm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cn_type</span> <span class="o">=</span> <span class="n">cn_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hard_out</span> <span class="o">=</span> <span class="n">hard_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_track_exit</span> <span class="o">=</span> <span class="n">track_exit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span> <span class="o">=</span> <span class="n">stateful</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span> <span class="o">=</span> <span class="n">output_dtype</span>

        <span class="c1"># clipping value for the atanh function is applied (tf.float32 is used)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_atanh_clip_value</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-7</span>
        <span class="c1"># clipping for min-sum decoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_llr_max_minsum</span> <span class="o">=</span> <span class="mi">20</span>

        <span class="c1"># init code parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_cns</span> <span class="o">=</span> <span class="n">pcm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># total number of check nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_vns</span> <span class="o">=</span> <span class="n">pcm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># total number of variable nodes</span>

        <span class="c1"># make pcm sparse first if ndarray is provided</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">pcm</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">pcm</span><span class="p">)</span>
        <span class="c1"># find all edges from variable and check node perspective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cn_con</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vn_con</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">pcm</span><span class="p">)</span>

        <span class="c1"># number of edges equals number of non-zero elements in the</span>
        <span class="c1"># parity-check matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_vn_con</span><span class="p">)</span>

        <span class="c1"># permutation index to rearrange messages into check node perspective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ind_cn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cn_con</span><span class="p">)</span>

        <span class="c1"># inverse permutation index to rearrange messages back into variable</span>
        <span class="c1"># node perspective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ind_cn_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ind_cn</span><span class="p">)</span>

        <span class="c1"># generate row masks (array of integers defining the row split pos.)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vn_row_splits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_node_mask_row</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_vn_con</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cn_row_splits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_node_mask_row</span><span class="p">(</span>
                                                    <span class="bp">self</span><span class="o">.</span><span class="n">_cn_con</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_ind_cn</span><span class="p">])</span>
        <span class="c1"># pre-load the CN function for performance reasons</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_type</span><span class="o">==</span><span class="s1">&#39;boxplus&#39;</span><span class="p">:</span>
            <span class="c1"># check node update using the tanh function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update_tanh</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_type</span><span class="o">==</span><span class="s1">&#39;boxplus-phi&#39;</span><span class="p">:</span>
            <span class="c1"># check node update using the &quot;_phi&quot; function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update_phi</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_type</span><span class="o">==</span><span class="s1">&#39;minsum&#39;</span><span class="p">:</span>
            <span class="c1"># check node update using the min-sum approximation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update_minsum</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown node type.&#39;</span><span class="p">)</span>

        <span class="c1"># init trainable weights if needed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_has_weights</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># indicates if trainable weights exist</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_has_weights</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_edge_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">),</span>
                                             <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span><span class="p">,</span>
                                             <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># track mutual information during decoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ie_c</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ie_v</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">#########################################</span>
    <span class="c1"># Public methods and properties</span>
    <span class="c1">#########################################</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pcm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Parity-check matrix of LDPC code.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pcm</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_cns</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Number of check nodes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_cns</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_vns</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Number of variable nodes.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_vns</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Number of edges in decoding graph.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">has_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Indicates if decoder has trainable weights.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edge_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Trainable weights of the BP decoder.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_weights</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Output dtype of decoder.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ie_c</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Extrinsic mutual information at check node.&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ie_c</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ie_v</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Extrinsic mutual information at variable node.&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ie_v</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Number of decoding iterations.&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span>

    <span class="nd">@num_iter</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">num_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_iter</span><span class="p">):</span>
        <span class="s2">&quot;Number of decoding iterations.&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="s1">&#39;num_iter must be int.&#39;</span>
        <span class="k">assert</span> <span class="n">num_iter</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;num_iter cannot be negative.&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

<div class="viewcode-block" id="LDPCBPDecoder.show_weights"><a class="viewcode-back" href="../../../../api/fec.ldpc.html#sionna.fec.ldpc.decoding.LDPCBPDecoder.show_weights">[docs]</a>    <span class="k">def</span> <span class="nf">show_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Show histogram of trainable weights.</span>

<span class="sd">        Input</span>
<span class="sd">        -----</span>
<span class="sd">            size: float</span>
<span class="sd">                Figure size of the matplotlib figure.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># only plot if weights exist</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_weights</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_weights</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;weight value&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Weight Distribution&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No weights to show.&quot;</span><span class="p">)</span></div>

    <span class="c1">#########################</span>
    <span class="c1"># Utility methods</span>
    <span class="c1">#########################</span>

    <span class="k">def</span> <span class="nf">_gen_node_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">con</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Generates internal node masks indicating which msg index belongs</span>
<span class="sd">        to which node index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">con</span><span class="p">)</span>
        <span class="n">con</span> <span class="o">=</span> <span class="n">con</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>

        <span class="n">node_mask</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">cur_node</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">cur_mask</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">con</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">cur_node</span><span class="p">:</span>
                <span class="n">cur_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">node_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mask</span><span class="p">)</span>
                <span class="n">cur_mask</span> <span class="o">=</span> <span class="p">[</span><span class="n">ind</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
                <span class="n">cur_node</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">node_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">node_mask</span>

    <span class="k">def</span> <span class="nf">_gen_node_mask_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">con</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Defining the row split positions of a 1D vector consisting of all</span>
<span class="sd">        edges messages.</span>

<span class="sd">        Used to build a ragged Tensor of incoming node messages.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">node_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># the first element indicates the first node index (=0)</span>

        <span class="n">cur_node</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">con</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">cur_node</span><span class="p">:</span>
                <span class="n">node_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">cur_node</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">node_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">)</span> <span class="c1"># last element must be the number of</span>
        <span class="c1"># elements (delimiter)</span>
        <span class="k">return</span> <span class="n">node_mask</span>

    <span class="k">def</span> <span class="nf">_vn_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">,</span> <span class="n">llr_ch</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Variable node update function.</span>

<span class="sd">        This function implements the (extrinsic) variable node update</span>
<span class="sd">        function. It takes the sum over all incoming messages ``msg`` excluding</span>
<span class="sd">        the intrinsic (= outgoing) message itself.</span>

<span class="sd">        Additionally, the channel LLR ``llr_ch`` is added to each message.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># aggregate all incoming messages per node</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">llr_ch</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the addition of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># subtract extrinsic message from node value</span>
        <span class="c1"># x = tf.expand_dims(x, axis=1)</span>
        <span class="c1"># x = tf.add(-msg, x)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span> <span class="p">:</span>
                                      <span class="n">x</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">),</span>
                                      <span class="o">-</span><span class="mf">1.</span><span class="o">*</span><span class="n">msg</span><span class="p">,</span>
                                      <span class="n">x</span><span class="p">,</span>
                                      <span class="n">msg</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_extrinsic_min</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Provides the extrinsic min operation for the minsum approximation</span>
<span class="sd">        of the CN function.</span>

<span class="sd">        This function implements the extrinsic min operation, i.e.,</span>
<span class="sd">        the min is taken over all values excluding the value at the current</span>
<span class="sd">        index.</span>

<span class="sd">        Note that the input is expected to be a Tensor and NOT a ragged Tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">num_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">msg</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">id_mat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_val</span><span class="p">)</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_val</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># create outgoing tensor per value</span>
               <span class="o">+</span> <span class="mf">1000.</span> <span class="o">*</span> <span class="n">id_mat</span><span class="p">)</span> <span class="c1"># &quot;ignore&quot; intrinsic msg by adding large const.</span>


        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">msg</span>

    <span class="k">def</span> <span class="nf">_where_ragged</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper to replace 0 elements from ragged tensor (called with</span>
<span class="sd">        map_flat_values).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-12</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_where_ragged_inv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper to replace small elements from ragged tensor (called with</span>
<span class="sd">        map_flat_values) with exact `0`.&quot;&quot;&quot;</span>
        <span class="n">msg_mod</span> <span class="o">=</span>  <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">msg</span><span class="p">),</span> <span class="mf">1e-7</span><span class="p">),</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">msg</span><span class="p">),</span>
                            <span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">msg_mod</span>

    <span class="k">def</span> <span class="nf">_cn_update_tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check node update function implementing the exact boxplus operation.</span>

<span class="sd">        This function implements the (extrinsic) check node update</span>
<span class="sd">        function. It calculates the boxplus function over all incoming messages</span>
<span class="sd">        &quot;msg&quot; excluding the intrinsic (=outgoing) message itself.</span>
<span class="sd">        The exact boxplus function is implemented by using the tanh function.</span>

<span class="sd">        The input is expected to be a ragged Tensor of shape</span>
<span class="sd">        `[num_cns, None, batch_size]`.</span>

<span class="sd">        Note that for numerical stability clipping is applied.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">msg</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="c1"># tanh is not overloaded for ragged tensors</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span> <span class="c1"># tanh is not overloaded</span>

        <span class="c1"># for ragged tensors; map to flat tensor first</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_where_ragged</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="n">msg_prod</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_prod</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the multiplication of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># ^-1 to avoid division</span>
        <span class="c1"># Note this is (potentially) numerically unstable</span>
        <span class="c1"># msg = msg**-1 * tf.expand_dims(msg_prod, axis=1) # remove own edge</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span> <span class="p">:</span>
                                        <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">),</span>
                                        <span class="n">msg</span><span class="o">**-</span><span class="mi">1</span><span class="p">,</span>
                                        <span class="n">msg_prod</span><span class="p">,</span>
                                        <span class="n">msg</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>

        <span class="c1"># Overwrite small (numerical zeros) message values with exact zero</span>
        <span class="c1"># these are introduced by the previous &quot;_where_ragged&quot; operation</span>
        <span class="c1"># this is required to keep the product stable (cf. _phi_update for log</span>
        <span class="c1"># sum implementation)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_where_ragged_inv</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span>
                               <span class="n">clip_value_min</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">_atanh_clip_value</span><span class="p">,</span>
                               <span class="n">clip_value_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_atanh_clip_value</span><span class="p">)</span>

        <span class="c1"># atanh is not overloaded for ragged tensors</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">atanh</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">msg</span>

    <span class="k">def</span> <span class="nf">_phi</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper function for the check node update.</span>

<span class="sd">        This function implements the (element-wise) `&quot;_phi&quot;` function as defined</span>
<span class="sd">        in [Ryan]_.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># the clipping values are optimized for tf.float32</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">clip_value_min</span><span class="o">=</span><span class="mf">8.5e-8</span><span class="p">,</span> <span class="n">clip_value_max</span><span class="o">=</span><span class="mf">16.635532</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_cn_update_phi</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check node update function implementing the exact boxplus operation.</span>

<span class="sd">        This function implements the (extrinsic) check node update function</span>
<span class="sd">        based on the numerically more stable `&quot;_phi&quot;` function (cf. [Ryan]_).</span>
<span class="sd">        It calculates the boxplus function over all incoming messages ``msg``</span>
<span class="sd">        excluding the intrinsic (=outgoing) message itself.</span>
<span class="sd">        The exact boxplus function is implemented by using the `&quot;_phi&quot;` function</span>
<span class="sd">        as in [Ryan]_.</span>

<span class="sd">        The input is expected to be a ragged Tensor of shape</span>
<span class="sd">        `[num_cns, None, batch_size]`.</span>

<span class="sd">        Note that for numerical stability clipping is applied.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">sign_val</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sign_val</span><span class="p">),</span>
                            <span class="n">sign_val</span><span class="p">)</span>

        <span class="n">sign_node</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_prod</span><span class="p">(</span><span class="n">sign_val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the multiplication of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># sign_val = sign_val * tf.expand_dims(sign_node, axis=1)</span>
        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span> <span class="p">:</span>
                                             <span class="n">x</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">),</span>
                                             <span class="n">sign_val</span><span class="p">,</span>
                                             <span class="n">sign_node</span><span class="p">,</span>
                                             <span class="n">sign_val</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span> <span class="c1"># remove sign</span>

        <span class="c1"># apply _phi element-wise (does not support ragged Tensors)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_phi</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">msg_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the addition of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># msg = tf.add( -msg, tf.expand_dims(msg_sum, axis=1)) # remove own edge</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span> <span class="p">:</span>
                                        <span class="n">x</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">),</span>
                                        <span class="o">-</span><span class="mf">1.</span><span class="o">*</span><span class="n">msg</span><span class="p">,</span>
                                        <span class="n">msg_sum</span><span class="p">,</span>
                                        <span class="n">msg</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>

        <span class="c1"># apply _phi element-wise (does not support ragged Tensors)</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_ragged_gradient</span><span class="p">(</span><span class="n">sign_val</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span>
                                                            <span class="bp">self</span><span class="o">.</span><span class="n">_phi</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">msg</span>

    <span class="k">def</span> <span class="nf">_stop_ragged_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rt</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper function as TF 2.5 does not support ragged gradient</span>
<span class="sd">        stopping&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">rt</span><span class="o">.</span><span class="n">with_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">rt</span><span class="o">.</span><span class="n">flat_values</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_sign_val_minsum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper to replace find sign-value during min-sum decoding.</span>
<span class="sd">        Must be called with `map_flat_values`.&quot;&quot;&quot;</span>

        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">sign_val</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sign_val</span><span class="p">),</span>
                            <span class="n">sign_val</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sign_val</span>

    <span class="k">def</span> <span class="nf">_cn_update_minsum_mapfn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Check node update function implementing the min-sum approximation.</span>

<span class="sd">        This function approximates the (extrinsic) check node update</span>
<span class="sd">        function based on the min-sum approximation (cf. [Ryan]_).</span>
<span class="sd">        It calculates the &quot;extrinsic&quot; min function over all incoming messages</span>
<span class="sd">        ``msg`` excluding the intrinsic (=outgoing) message itself.</span>

<span class="sd">        The input is expected to be a ragged Tensor of shape</span>
<span class="sd">        `[num_vns, None, batch_size]`.</span>

<span class="sd">        This function uses tf.map_fn() to call the CN updates.</span>
<span class="sd">        It is currently not used, but can be used as template to implement</span>
<span class="sd">        modified CN functions (e.g., offset-corrected minsum).</span>
<span class="sd">        Please note that tf.map_fn lowers the throughput significantly.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sign_val_minsum</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="n">sign_node</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_prod</span><span class="p">(</span><span class="n">sign_val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sign_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_ragged_gradient</span><span class="p">(</span><span class="n">sign_val</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                                                             <span class="n">sign_node</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span> <span class="c1"># remove sign</span>

        <span class="c1"># calculate extrinsic messages and include the sign</span>
        <span class="n">msg_e</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">map_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_extrinsic_min</span><span class="p">,</span> <span class="n">msg</span><span class="p">,</span> <span class="n">infer_shape</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># ensure shape after map_fn</span>
        <span class="n">msg_fv</span> <span class="o">=</span> <span class="n">msg_e</span><span class="o">.</span><span class="n">flat_values</span>
        <span class="n">msg_fv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ensure_shape</span><span class="p">(</span><span class="n">msg_fv</span><span class="p">,</span> <span class="n">msg</span><span class="o">.</span><span class="n">flat_values</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">msg_e</span> <span class="o">=</span> <span class="n">msg</span><span class="o">.</span><span class="n">with_flat_values</span><span class="p">(</span><span class="n">msg_fv</span><span class="p">)</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">sign_val</span> <span class="o">*</span> <span class="n">msg_e</span>

        <span class="k">return</span> <span class="n">msg</span>

    <span class="k">def</span> <span class="nf">_cn_update_minsum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Check node update function implementing the min-sum approximation.</span>

<span class="sd">        This function approximates the (extrinsic) check node update</span>
<span class="sd">        function based on the min-sum approximation (cf. [Ryan]_).</span>
<span class="sd">        It calculates the &quot;extrinsic&quot; min function over all incoming messages</span>
<span class="sd">        ``msg`` excluding the intrinsic (=outgoing) message itself.</span>

<span class="sd">        The input is expected to be a ragged Tensor of shape</span>
<span class="sd">        `[num_vns, None, batch_size]`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># a constant used overwrite the first min</span>
        <span class="n">LARGE_VAL</span> <span class="o">=</span> <span class="mf">10000.</span> <span class="c1"># pylint: disable=invalid-name</span>

        <span class="c1"># clip values for numerical stability</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span>
                               <span class="n">clip_value_min</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">_llr_max_minsum</span><span class="p">,</span>
                               <span class="n">clip_value_max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_llr_max_minsum</span><span class="p">)</span>

        <span class="c1"># calculate sign of outgoing msg</span>
        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sign_val_minsum</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="n">sign_node</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_prod</span><span class="p">(</span><span class="n">sign_val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the multiplication of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># sign_val = self._stop_ragged_gradient(sign_val) \</span>
        <span class="c1">#             * tf.expand_dims(sign_node, axis=1)</span>
        <span class="n">sign_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span>
                                        <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">:</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">)),</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">_stop_ragged_gradient</span><span class="p">(</span><span class="n">sign_val</span><span class="p">),</span>
                                        <span class="n">sign_node</span><span class="p">,</span>
                                        <span class="n">sign_val</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span> <span class="c1"># remove sign</span>

        <span class="c1"># Calculate the extrinsic minimum per CN, i.e., for each message of</span>
        <span class="c1"># index i, find the smallest and the second smallest value.</span>
        <span class="c1"># However, in some cases the second smallest value may equal the</span>
        <span class="c1"># smallest value (multiplicity of mins).</span>
        <span class="c1"># Please note that this needs to be applied to raggedTensors, e.g.,</span>
        <span class="c1"># tf.top_k() is currently not supported and the ops must support graph</span>
        <span class="c1"># # mode.</span>

        <span class="c1"># find min_value per node</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the subtraction of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># and subtract min; the new array contains zero at the min positions</span>
        <span class="c1"># benefits from broadcasting; all other values are positive</span>
        <span class="c1"># msg_min1 = msg - min_val</span>
        <span class="n">msg_min1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">:</span>
                                             <span class="n">x</span><span class="o">-</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">row_ind</span><span class="p">),</span>
                                             <span class="n">msg</span><span class="p">,</span>
                                             <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                             <span class="n">msg</span><span class="o">.</span><span class="n">value_rowids</span><span class="p">())</span>

        <span class="c1"># replace 0 (=min positions) with large value to ignore it for further</span>
        <span class="c1"># min calculations</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span>
                                        <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">LARGE_VAL</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span>
                                        <span class="n">msg_min1</span><span class="p">)</span>

        <span class="c1"># find the second smallest element (we add min_val as this has been</span>
        <span class="c1"># subtracted before)</span>
        <span class="n">min_val2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">min_val</span>

        <span class="c1"># Detect duplicated minima (i.e., min_val occurs at two incoming</span>
        <span class="c1"># messages). As the LLRs per node are &lt;LLR_MAX and we have</span>
        <span class="c1"># replace at least 1 position (position with message &quot;min_val&quot;) by</span>
        <span class="c1"># LARGE_VAL, it holds for the sum &lt; LARGE_VAL + node_degree*LLR_MAX.</span>
        <span class="c1"># if the sum &gt; 2*LARGE_VAL, the multiplicity of the min is at least 2.</span>
        <span class="n">node_sum</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">LARGE_VAL</span><span class="o">-</span><span class="mf">1.</span><span class="p">)</span>
        <span class="c1"># indicator that duplicated min was detected (per node)</span>
        <span class="n">double_min</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">node_sum</span><span class="p">))</span>

        <span class="c1"># if a duplicate min occurred, both edges must have min_val, otherwise</span>
        <span class="c1"># the second smallest value is taken</span>
        <span class="n">min_val_e</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">double_min</span><span class="p">)</span> <span class="o">*</span> <span class="n">min_val</span> <span class="o">+</span> <span class="p">(</span><span class="n">double_min</span><span class="p">)</span> <span class="o">*</span> <span class="n">min_val2</span>

        <span class="c1"># replace all values with min_val except the position where the min</span>
        <span class="c1"># occurred (=extrinsic min).</span>
        <span class="n">msg_e</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">msg</span><span class="o">==</span><span class="n">LARGE_VAL</span><span class="p">,</span> <span class="n">min_val_e</span><span class="p">,</span> <span class="n">min_val</span><span class="p">)</span>

        <span class="c1"># it seems like tf.where does not set the shape of tf.ragged properly</span>
        <span class="c1"># we need to ensure the shape manually</span>
        <span class="n">msg_e</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span>
                                    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span>
                                    <span class="n">tf</span><span class="o">.</span><span class="n">ensure_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">msg</span><span class="o">.</span><span class="n">flat_values</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                                    <span class="n">msg_e</span><span class="p">)</span>

        <span class="c1"># TF2.9 does not support XLA for the multiplication of ragged tensors</span>
        <span class="c1"># the following code provides a workaround that supports XLA</span>

        <span class="c1"># and apply sign</span>
        <span class="c1">#msg = sign_val * msg_e</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">multiply</span><span class="p">,</span>
                                        <span class="n">sign_val</span><span class="p">,</span>
                                        <span class="n">msg_e</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">msg</span>

    <span class="k">def</span> <span class="nf">_mult_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Multiply messages with trainable weights for weighted BP.&quot;&quot;&quot;</span>
        <span class="c1"># transpose for simpler broadcasting of training variables</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_edge_weights</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="c1">#########################</span>
    <span class="c1"># Keras layer functions</span>
    <span class="c1">#########################</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="c1"># Raise AssertionError if shape of x is invalid</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
            <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">),</span> \
                <span class="s2">&quot;For stateful decoding, a tuple of two inputs is expected.&quot;</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">assert</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_vns</span><span class="p">),</span> \
                            <span class="s1">&#39;Last dimension must be of length n.&#39;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;The inputs must have at least rank 2.&#39;</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterative BP decoding function.</span>

<span class="sd">        This function performs ``num_iter`` belief propagation decoding</span>
<span class="sd">        iterations and returns the estimated codeword.</span>

<span class="sd">        Args:</span>
<span class="sd">        llr_ch or (llr_ch, msg_vn):</span>

<span class="sd">            llr_ch (tf.float32): Tensor of shape `[...,n]` containing the</span>
<span class="sd">                channel logits/llr values.</span>

<span class="sd">            msg_vn (tf.float32) : Ragged tensor containing the VN</span>
<span class="sd">                messages, or None. Required if ``stateful`` is set to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `tf.float32`: Tensor of shape `[...,n]` containing</span>
<span class="sd">            bit-wise soft-estimates (or hard-decided bit-values) of all</span>
<span class="sd">            codeword bits.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If ``inputs`` is not of shape `[batch_size, n]`.</span>

<span class="sd">            InvalidArgumentError: When rank(``inputs``)&lt;2.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Extract inputs</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
            <span class="n">llr_ch</span><span class="p">,</span> <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">llr_ch</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_type</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;Invalid input dtype.&#39;</span><span class="p">)</span>

        <span class="c1"># internal calculations still in tf.float32</span>
        <span class="n">llr_ch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># last dim must be of length n</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">_num_vns</span><span class="p">,</span>
                                  <span class="s1">&#39;Last dimension must be of length n.&#39;</span><span class="p">)</span>

        <span class="n">llr_ch_shape</span> <span class="o">=</span> <span class="n">llr_ch</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_vns</span><span class="p">]</span>
        <span class="n">llr_ch_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>

        <span class="c1"># must be done during call, as XLA fails otherwise due to ragged</span>
        <span class="c1"># indices placed on the CPU device.</span>
        <span class="c1"># create permutation index from cn perspective</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cn_mask_tf</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gen_node_mask</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cn_con</span><span class="p">),</span>
                                              <span class="n">row_splits_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

        <span class="c1"># batch dimension is last dimension due to ragged tensor representation</span>
        <span class="n">llr_ch</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">llr_ch_reshaped</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

        <span class="n">llr_ch</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">llr_ch</span> <span class="c1"># logits are converted into &quot;true&quot; llrs</span>

        <span class="c1"># init internal decoder state if not explicitly</span>
        <span class="c1"># provided (e.g., required to restore decoder state for iterative</span>
        <span class="c1"># detection and decoding)</span>
        <span class="c1"># load internal state from previous iteration</span>
        <span class="c1"># required for iterative det./dec.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span> <span class="ow">or</span> <span class="n">msg_vn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">msg_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">),</span>
                                   <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">)[</span><span class="mi">1</span><span class="p">]],</span>
                                   <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">msg_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">msg_vn</span><span class="o">.</span><span class="n">flat_values</span>

        <span class="c1"># track exit decoding trajectory; requires all-zero cw?</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_track_exit</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ie_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_ie_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># perform one decoding iteration</span>
        <span class="c1"># Remark: msg_vn cannot be ragged as input for tf.while_loop as</span>
        <span class="c1"># otherwise XLA will not be supported (with TF 2.5)</span>
        <span class="k">def</span> <span class="nf">dec_iter</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">msg_vn</span><span class="p">,</span> <span class="n">it</span><span class="p">):</span>
            <span class="n">it</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span>
                        <span class="n">values</span><span class="o">=</span><span class="n">msg_vn</span><span class="p">,</span>
                        <span class="n">row_splits</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_vn_row_splits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
            <span class="c1"># variable node update</span>
            <span class="n">msg_vn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vn_update</span><span class="p">(</span><span class="n">msg_vn</span><span class="p">,</span> <span class="n">llr_ch</span><span class="p">)</span>

            <span class="c1"># track exit decoding trajectory; requires all-zero cw</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_track_exit</span><span class="p">:</span>
                <span class="c1"># neg values as different llr def is expected</span>
                <span class="n">mi</span> <span class="o">=</span> <span class="n">llr2mi</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">msg_vn</span><span class="o">.</span><span class="n">flat_values</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ie_v</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ie_v</span><span class="p">,</span>
                                                     <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                     <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mi</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

            <span class="c1"># scale outgoing vn messages (weighted BP); only if activated</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_has_weights</span><span class="p">:</span>
                <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ragged</span><span class="o">.</span><span class="n">map_flat_values</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mult_weights</span><span class="p">,</span>
                                                   <span class="n">msg_vn</span><span class="p">)</span>
            <span class="c1"># permute edges into CN perspective</span>
            <span class="n">msg_cn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">msg_vn</span><span class="o">.</span><span class="n">flat_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_mask_tf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># check node update using the pre-defined function</span>
            <span class="n">msg_cn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cn_update</span><span class="p">(</span><span class="n">msg_cn</span><span class="p">)</span>

            <span class="c1"># track exit decoding trajectory; requires all-zero cw?</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_track_exit</span><span class="p">:</span>
                <span class="c1"># neg values as different llr def is expected</span>
                <span class="n">mi</span> <span class="o">=</span> <span class="n">llr2mi</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="o">*</span><span class="n">msg_cn</span><span class="o">.</span><span class="n">flat_values</span><span class="p">)</span>
                <span class="c1"># update pos i+1 such that first iter is stored as 0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ie_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ie_c</span><span class="p">,</span>
                                                     <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                     <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mi</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

            <span class="c1"># re-permute edges to variable node perspective</span>
            <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">msg_cn</span><span class="o">.</span><span class="n">flat_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ind_cn_inv</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">llr_ch</span><span class="p">,</span> <span class="n">msg_vn</span><span class="p">,</span> <span class="n">it</span>

        <span class="c1"># stopping condition (required for tf.while_loop)</span>
        <span class="k">def</span> <span class="nf">dec_stop</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">msg_vn</span><span class="p">,</span> <span class="n">it</span><span class="p">):</span> <span class="c1"># pylint: disable=W0613</span>
            <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span><span class="p">)</span>

        <span class="c1"># start decoding iterations</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># maximum_iterations required for XLA</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">msg_vn</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="n">dec_stop</span><span class="p">,</span>
                                     <span class="n">dec_iter</span><span class="p">,</span>
                                     <span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">msg_vn</span><span class="p">,</span> <span class="n">it</span><span class="p">),</span>
                                     <span class="n">parallel_iterations</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                     <span class="n">maximum_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_iter</span><span class="p">)</span>


        <span class="c1"># raggedTensor for final marginalization</span>
        <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">RaggedTensor</span><span class="o">.</span><span class="n">from_row_splits</span><span class="p">(</span>
                        <span class="n">values</span><span class="o">=</span><span class="n">msg_vn</span><span class="p">,</span>
                        <span class="n">row_splits</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_vn_row_splits</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

        <span class="c1"># marginalize and remove ragged Tensor</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">msg_vn</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># restore batch dimension to first dimension</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>

        <span class="n">x_hat</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">x_hat</span> <span class="c1"># convert llrs back into logits</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hard_out</span><span class="p">:</span> <span class="c1"># hard decide decoder output if required</span>
            <span class="n">x_hat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">)</span>

        <span class="c1"># Reshape c_short so that it matches the original input dimensions</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="n">llr_ch_shape</span>
        <span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1"># overwrite batch dim (can be None in Keras)</span>

        <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>

        <span class="c1"># cast output to output_dtype</span>
        <span class="n">x_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_reshaped</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_out</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">msg_vn</span></div>

<div class="viewcode-block" id="LDPC5GDecoder"><a class="viewcode-back" href="../../../../api/fec.ldpc.html#sionna.fec.ldpc.decoding.LDPC5GDecoder">[docs]</a><span class="k">class</span> <span class="nc">LDPC5GDecoder</span><span class="p">(</span><span class="n">LDPCBPDecoder</span><span class="p">):</span>
    <span class="c1"># pylint: disable=line-too-long</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;LDPC5GDecoder(encoder, trainable=False, cn_type=&#39;boxplus-phi&#39;, hard_out=True, track_exit=False, return_infobits=True, prune_pcm=True, num_iter=20, stateful=False, output_dtype=tf.float32, **kwargs)</span>

<span class="sd">    (Iterative) belief propagation decoder for 5G NR LDPC codes.</span>

<span class="sd">    Inherits from :class:`~sionna.fec.ldpc.decoding.LDPCBPDecoder` and provides</span>
<span class="sd">    a wrapper for 5G compatibility, i.e., automatically handles puncturing and</span>
<span class="sd">    shortening according to [3GPPTS38212_LDPC]_.</span>

<span class="sd">    Note that for full 5G 3GPP NR compatibility, the correct puncturing and</span>
<span class="sd">    shortening patterns must be applied and, thus, the encoder object is</span>
<span class="sd">    required as input.</span>

<span class="sd">    If required the decoder can be made trainable and is differentiable</span>
<span class="sd">    (the training of some check node types may be not supported) following the</span>
<span class="sd">    concept of &quot;weighted BP&quot; [Nachmani]_.</span>

<span class="sd">    The class inherits from the Keras layer class and can be used as layer in a</span>
<span class="sd">    Keras model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">        encoder: LDPC5GEncoder</span>
<span class="sd">            An instance of :class:`~sionna.fec.ldpc.encoding.LDPC5GEncoder`</span>
<span class="sd">            containing the correct code parameters.</span>

<span class="sd">        trainable: bool</span>
<span class="sd">            Defaults to False. If True, every outgoing variable node message is</span>
<span class="sd">            scaled with a trainable scalar.</span>

<span class="sd">        cn_type: str</span>
<span class="sd">            A string defaults to &#39;&quot;boxplus-phi&quot;&#39;. One of</span>
<span class="sd">            {`&quot;boxplus&quot;`, `&quot;boxplus-phi&quot;`, `&quot;minsum&quot;`} where</span>
<span class="sd">            &#39;&quot;boxplus&quot;&#39; implements the single-parity-check APP decoding rule.</span>
<span class="sd">            &#39;&quot;boxplus-phi&quot;&#39; implements the numerical more stable version of</span>
<span class="sd">            boxplus [Ryan]_.</span>
<span class="sd">            &#39;&quot;minsum&quot;&#39; implements the min-approximation of the CN</span>
<span class="sd">            update rule [Ryan]_.</span>

<span class="sd">        hard_out: bool</span>
<span class="sd">            Defaults to True. If True, the decoder provides hard-decided</span>
<span class="sd">            codeword bits instead of soft-values.</span>

<span class="sd">        track_exit: bool</span>
<span class="sd">            Defaults to False. If True, the decoder tracks EXIT characteristics.</span>
<span class="sd">            Note that this requires the all-zero CW as input.</span>

<span class="sd">        return_infobits: bool</span>
<span class="sd">            Defaults to True. If True, only the `k` info bits (soft or</span>
<span class="sd">            hard-decided) are returned. Otherwise all `n` positions are</span>
<span class="sd">            returned.</span>

<span class="sd">        prune_pcm: bool</span>
<span class="sd">            Defaults to True. If True, all punctured degree-1 VNs and</span>
<span class="sd">            connected check nodes are removed from the decoding graph (see</span>
<span class="sd">            [Cammerer]_ for details). Besides numerical differences, this should</span>
<span class="sd">            yield the same decoding result but improved the decoding throughput</span>
<span class="sd">            and reduces the memory footprint.</span>

<span class="sd">        num_iter: int</span>
<span class="sd">            Defining the number of decoder iteration (no early stopping used at</span>
<span class="sd">            the moment!).</span>

<span class="sd">        stateful: bool</span>
<span class="sd">            Defaults to False. If True, the internal VN messages ``msg_vn``</span>
<span class="sd">            from the last decoding iteration are returned, and ``msg_vn`` or</span>
<span class="sd">            `None` needs to be given as a second input when calling the decoder.</span>
<span class="sd">            This is required for iterative demapping and decoding.</span>

<span class="sd">        output_dtype: tf.DType</span>
<span class="sd">            Defaults to tf.float32. Defines the output datatype of the layer</span>
<span class="sd">            (internal precision remains tf.float32).</span>

<span class="sd">    Input</span>
<span class="sd">    -----</span>
<span class="sd">    llrs_ch or (llrs_ch, msg_vn):</span>
<span class="sd">        Tensor or Tuple (only required if ``stateful`` is True):</span>

<span class="sd">    llrs_ch: [...,n], tf.float32</span>
<span class="sd">        2+D tensor containing the channel logits/llr values.</span>

<span class="sd">    msg_vn: None or RaggedTensor, tf.float32</span>
<span class="sd">        Ragged tensor of VN messages.</span>
<span class="sd">        Required only if ``stateful`` is True.</span>

<span class="sd">    Output</span>
<span class="sd">    ------</span>
<span class="sd">        : [...,n] or [...,k], tf.float32</span>
<span class="sd">            2+D Tensor of same shape as ``inputs`` containing</span>
<span class="sd">            bit-wise soft-estimates (or hard-decided bit-values) of all</span>
<span class="sd">            codeword bits. If ``return_infobits`` is True, only the `k`</span>
<span class="sd">            information bits are returned.</span>

<span class="sd">        : RaggedTensor, tf.float32:</span>
<span class="sd">            Tensor of VN messages.</span>
<span class="sd">            Returned only if ``stateful`` is set to True.</span>
<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If the shape of ``pcm`` is invalid or contains other</span>
<span class="sd">            values than `0` or `1`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``trainable`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``track_exit`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``hard_out`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``return_infobits`` is not `bool`.</span>

<span class="sd">        AssertionError</span>
<span class="sd">            If ``encoder`` is not an instance of</span>
<span class="sd">            :class:`~sionna.fec.ldpc.encoding.LDPC5GEncoder`.</span>

<span class="sd">        ValueError</span>
<span class="sd">            If ``output_dtype`` is not {tf.float16, tf.float32, tf.</span>
<span class="sd">            float64}.</span>

<span class="sd">        ValueError</span>
<span class="sd">            If ``inputs`` is not of shape `[batch_size, n]`.</span>

<span class="sd">        ValueError</span>
<span class="sd">            If ``num_iter`` is not an integer greater (or equal) `0`.</span>

<span class="sd">        InvalidArgumentError</span>
<span class="sd">            When rank(``inputs``)&lt;2.</span>

<span class="sd">    Note</span>
<span class="sd">    ----</span>
<span class="sd">        As decoding input logits</span>
<span class="sd">        :math:`\operatorname{log} \frac{p(x=1)}{p(x=0)}` are assumed for</span>
<span class="sd">        compatibility with the learning framework, but</span>
<span class="sd">        internally llrs with definition</span>
<span class="sd">        :math:`\operatorname{log} \frac{p(x=0)}{p(x=1)}` are used.</span>

<span class="sd">        The decoder is not (particularly) optimized for Quasi-cyclic (QC) LDPC</span>
<span class="sd">        codes and, thus, supports arbitrary parity-check matrices.</span>

<span class="sd">        The decoder is implemented by using &#39;&quot;ragged Tensors&quot;&#39; [TF_ragged]_ to</span>
<span class="sd">        account for arbitrary node degrees. To avoid a performance degradation</span>
<span class="sd">        caused by a severe indexing overhead, the batch-dimension is shifted to</span>
<span class="sd">        the last dimension during decoding.</span>

<span class="sd">        If the decoder is made trainable [Nachmani]_, for performance</span>
<span class="sd">        improvements only variable to check node messages are scaled as the VN</span>
<span class="sd">        operation is linear and, thus, would not increase the expressive power</span>
<span class="sd">        of the weights.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">encoder</span><span class="p">,</span>
                 <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cn_type</span><span class="o">=</span><span class="s1">&#39;boxplus-phi&#39;</span><span class="p">,</span>
                 <span class="n">hard_out</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">track_exit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">return_infobits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">prune_pcm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">num_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">stateful</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">output_dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="c1"># needs the 5G Encoder to access all 5G parameters</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">LDPC5GEncoder</span><span class="p">),</span> <span class="s1">&#39;encoder must </span><span class="se">\</span>
<span class="s1">                          be of class LDPC5GEncoder.&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="n">pcm</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">pcm</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">return_infobits</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;return_info must be bool.&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_return_infobits</span> <span class="o">=</span> <span class="n">return_infobits</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_llr_max</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># internal max value for LLR initialization</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_dtype</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">DType</span><span class="p">),</span> \
                                <span class="s1">&#39;output_dtype must be tf.DType.&#39;</span>
        <span class="k">if</span> <span class="n">output_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;output_dtype must be {tf.float16, tf.float32, tf.float64}.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span> <span class="o">=</span> <span class="n">output_dtype</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stateful</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;stateful must be bool.&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span> <span class="o">=</span> <span class="n">stateful</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prune_pcm</span><span class="p">,</span> <span class="nb">bool</span><span class="p">),</span> <span class="s1">&#39;prune_pcm must be bool.&#39;</span>
        <span class="c1"># prune punctured degree-1 VNs and connected CNs. A punctured</span>
        <span class="c1"># VN-1 node will always &quot;send&quot; llr=0 to the connected CN. Thus, this</span>
        <span class="c1"># CN will only send 0 messages to all other VNs, i.e., does not</span>
        <span class="c1"># contribute to the decoding process.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prune_pcm</span> <span class="o">=</span> <span class="n">prune_pcm</span>
        <span class="k">if</span> <span class="n">prune_pcm</span><span class="p">:</span>
            <span class="c1"># find index of first position with only degree-1 VN</span>
            <span class="n">dv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># VN degree</span>
            <span class="n">last_pos</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">_n_ldpc</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">_n_ldpc</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">dv</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">last_pos</span> <span class="o">=</span> <span class="n">idx</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">break</span>
            <span class="c1"># number of filler bits</span>
            <span class="n">k_filler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k_ldpc</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span>
            <span class="c1"># number of punctured bits</span>
            <span class="n">nb_punc_bits</span> <span class="o">=</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n_ldpc</span> <span class="o">-</span> <span class="n">k_filler</span><span class="p">)</span>
                                     <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>
            <span class="c1"># effective codeword length after pruning of vn-1 nodes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_pruned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="n">last_pos</span><span class="p">,</span> <span class="n">encoder</span><span class="o">.</span><span class="n">_n_ldpc</span> <span class="o">-</span> <span class="n">nb_punc_bits</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">_n_ldpc</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_pruned</span>
            <span class="c1"># remove last CNs and VNs from pcm</span>
            <span class="n">pcm</span> <span class="o">=</span> <span class="n">pcm</span><span class="p">[:</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span><span class="p">]</span>

            <span class="c1">#check for consistency</span>
            <span class="k">assert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">),</span> <span class="s2">&quot;Internal error: number of </span><span class="se">\</span>
<span class="s2">                        pruned nodes must be positive.&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># no pruning; same length as before</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_n_pruned</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">_n_ldpc</span>



        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span>
                         <span class="n">trainable</span><span class="p">,</span>
                         <span class="n">cn_type</span><span class="p">,</span>
                         <span class="n">hard_out</span><span class="p">,</span>
                         <span class="n">track_exit</span><span class="p">,</span>
                         <span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span><span class="p">,</span>
                         <span class="n">stateful</span><span class="o">=</span><span class="n">stateful</span><span class="p">,</span>
                         <span class="n">output_dtype</span><span class="o">=</span><span class="n">output_dtype</span><span class="p">,</span>
                         <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1">#########################################</span>
    <span class="c1"># Public methods and properties</span>
    <span class="c1">#########################################</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">llr_max</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Max LLR value used for rate-matching.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_llr_max</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;LDPC Encoder used for rate-matching/recovery.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span>

    <span class="c1">#########################</span>
    <span class="c1"># Keras layer functions</span>
    <span class="c1">#########################</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
            <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">),</span> \
                <span class="s2">&quot;For stateful decoding, a tuple of two inputs is expected.&quot;</span>
            <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># check input dimensions for consistency</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n</span><span class="p">),</span> \
                                <span class="s1">&#39;Last dimension must be of length n.&#39;</span>
        <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">&gt;=</span><span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;The inputs must have at least rank 2.&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_old_shape_5g</span> <span class="o">=</span> <span class="n">input_shape</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterative BP decoding function.</span>

<span class="sd">        This function performs ``num_iter`` belief propagation decoding</span>
<span class="sd">        iterations and returns the estimated codeword.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (tf.float32): Tensor of shape `[...,n]` containing the</span>
<span class="sd">                channel logits/llr values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `tf.float32`: Tensor of shape `[...,n]` or `[...,k]`</span>
<span class="sd">            (``return_infobits`` is True) containing bit-wise soft-estimates</span>
<span class="sd">            (or hard-decided bit-values) of all codeword bits (or info</span>
<span class="sd">            bits, respectively).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If ``inputs`` is not of shape `[batch_size, n]`.</span>

<span class="sd">            ValueError: If ``num_iter`` is not an integer greater (or equal)</span>
<span class="sd">                `0`.</span>

<span class="sd">            InvalidArgumentError: When rank(``inputs``)&lt;2.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Extract inputs</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
            <span class="n">llr_ch</span><span class="p">,</span> <span class="n">msg_vn</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">llr_ch</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">debugging</span><span class="o">.</span><span class="n">assert_type</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s1">&#39;Invalid input dtype.&#39;</span><span class="p">)</span>

        <span class="n">llr_ch_shape</span> <span class="o">=</span> <span class="n">llr_ch</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">llr_ch_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">llr_ch_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">llr_ch</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">llr_ch_reshaped</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># invert if rate-matching output interleaver was applied as defined in</span>
        <span class="c1"># Sec. 5.4.2.2 in 38.212</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="o">.</span><span class="n">num_bits_per_symbol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">llr_ch_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">llr_ch_reshaped</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="o">.</span><span class="n">out_int_inv</span><span class="p">,</span>
                                        <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


        <span class="c1"># undo puncturing of the first 2*Z bit positions</span>
        <span class="n">llr_5g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">z</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">),</span>
                          <span class="n">llr_ch_reshaped</span><span class="p">],</span>
                          <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># undo puncturing of the last positions</span>
        <span class="c1"># total length must be n_ldpc, while llr_ch has length n</span>
        <span class="c1"># first 2*z positions are already added</span>
        <span class="c1"># -&gt; add n_ldpc - n - 2Z punctured positions</span>
        <span class="n">k_filler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k_ldpc</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span> <span class="c1"># number of filler bits</span>
        <span class="n">nb_punc_bits</span> <span class="o">=</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n_ldpc</span> <span class="o">-</span> <span class="n">k_filler</span><span class="p">)</span>
                                     <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>


        <span class="n">llr_5g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">llr_5g</span><span class="p">,</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">nb_punc_bits</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span><span class="p">],</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">)],</span>
                            <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># undo shortening (= add 0 positions after k bits, i.e. LLR=LLR_max)</span>
        <span class="c1"># the first k positions are the systematic bits</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">llr_5g</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span><span class="p">])</span>

        <span class="c1"># parity part</span>
        <span class="n">nb_par_bits</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n_ldpc</span> <span class="o">-</span> <span class="n">k_filler</span>
                       <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nb_pruned_nodes</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">llr_5g</span><span class="p">,</span>
                      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span><span class="p">],</span>
                      <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">nb_par_bits</span><span class="p">])</span>

        <span class="c1"># negative sign due to logit definition</span>
        <span class="n">z</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_llr_max</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">k_filler</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">)</span>

        <span class="n">llr_5g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># and execute the decoder</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
            <span class="n">x_hat</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">llr_5g</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_hat</span><span class="p">,</span><span class="n">msg_vn</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">call</span><span class="p">([</span><span class="n">llr_5g</span><span class="p">,</span> <span class="n">msg_vn</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_return_infobits</span><span class="p">:</span> <span class="c1"># return only info bits</span>
            <span class="c1"># reconstruct u_hat # code is systematic</span>
            <span class="n">u_hat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span><span class="p">])</span>
            <span class="c1"># Reshape u_hat so that it matches the original input dimensions</span>
            <span class="n">output_shape</span> <span class="o">=</span> <span class="n">llr_ch_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
            <span class="c1"># overwrite first dimension as this could be None (Keras)</span>
            <span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">u_reshaped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">u_hat</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>

            <span class="c1"># enable other output datatypes than tf.float32</span>
            <span class="n">u_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">u_reshaped</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">u_out</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">u_out</span><span class="p">,</span> <span class="n">msg_vn</span>

        <span class="k">else</span><span class="p">:</span> <span class="c1"># return all codeword bits</span>
            <span class="c1"># the transmitted CW bits are not the same as used during decoding</span>
            <span class="c1"># cf. last parts of 5G encoding function</span>

            <span class="c1"># remove last dim</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_pruned</span><span class="p">])</span>

            <span class="c1"># remove filler bits at pos (k, k_ldpc)</span>
            <span class="n">x_no_filler1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k</span><span class="p">])</span>

            <span class="n">x_no_filler2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k_ldpc</span><span class="p">],</span>
                                    <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">_n_pruned</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">k_ldpc</span><span class="p">])</span>

            <span class="n">x_no_filler</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x_no_filler1</span><span class="p">,</span> <span class="n">x_no_filler2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># shorten the first 2*Z positions and end after n bits</span>
            <span class="n">x_short</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">x_no_filler</span><span class="p">,</span>
                               <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">z</span><span class="p">],</span>
                               <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">n</span><span class="p">])</span>

            <span class="c1"># if used, apply rate-matching output interleaver again as</span>
            <span class="c1"># Sec. 5.4.2.2 in 38.212</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="o">.</span><span class="n">num_bits_per_symbol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x_short</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x_short</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="o">.</span><span class="n">out_int</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Reshape x_short so that it matches the original input dimensions</span>
            <span class="c1"># overwrite first dimension as this could be None (Keras)</span>
            <span class="n">llr_ch_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="n">x_short</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_short</span><span class="p">,</span> <span class="n">llr_ch_shape</span><span class="p">)</span>

            <span class="c1"># enable other output datatypes than tf.float32</span>
            <span class="n">x_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x_short</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stateful</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">x_out</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">msg_vn</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2022 NVIDIA CORPORATION.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

</body>
</html>