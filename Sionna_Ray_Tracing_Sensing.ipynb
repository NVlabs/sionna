{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6a8731",
   "metadata": {},
   "source": [
    "# Introduction to Sionna RT - Scene and Assets Configuration Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702817e",
   "metadata": {},
   "source": [
    "In this notebook, you will\n",
    "- Discover how to use the basic functionalities of Sionna's [ray tracing (RT) module](https://nvlabs.github.io/sionna/api/rt.html).\n",
    "- Learn how to programaticaly append or remove assets object to a scene (without using Blender).\n",
    "- See the impact of the assets within the scene w.r.t. ray-traced channels for link-level simulations instead of stochastic channel models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0a988",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Background Information](#Background-Information)\n",
    "* [GPU Configuration and Imports](#GPU-Configuration-and-Imports)\n",
    "* [Loading Scenes](#Loading-Scenes)\n",
    "* [Loading Assets](#Loading-Assets)\n",
    "* [Removing Assets](#Removing-Assets)\n",
    "* [Rendering images](#Rendering-images)\n",
    "* [Ray Tracing for Radio Propagation](#Ray-Tracing-for-Radio-Propagation)\n",
    "* [From Paths to Channel Impulse Responses](#From-Paths-to-Channel-Impulse-Responses)\n",
    "* [Coverage Map](#Coverage-Map)\n",
    "* [Conclusion and Outlook](#Conclusion-and-Outlook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32f3d7",
   "metadata": {},
   "source": [
    "## Background Information\n",
    "\n",
    "It is usefull to be able to construct a scene dynamically by adding and removing objects, referred to as assets, thus being able to automatically generate datasets and/or define complex scenario. To this end we propose a few functionalities that allows to do so. One can now define a scene, e.g. using Blender, importing the scene within Sionna and then being able to add asset objetcs (that can separately be defined in Blender too) to that scene. For example one could imagine to define a scene with a car park, and append to that scene a varing number of cars to see the impact in terms of RF propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938106f2",
   "metadata": {},
   "source": [
    "## GPU Configuration and Imports <a class=\"anchor\" id=\"GPU-Configuration-and-Imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c23abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_num = 0 # Use \"\" to use the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Colab does currently not support the latest version of ipython.\n",
    "# Thus, the preview does not work in Colab. However, whenever possible we\n",
    "# strongly recommend to use the scene preview mode.\n",
    "try: # detect if the notebook runs in Colab\n",
    "    import google.colab\n",
    "    colab_compat = True # deactivate preview\n",
    "except:\n",
    "    colab_compat = False\n",
    "resolution = [480,320] # increase for higher quality of renderings\n",
    "\n",
    "# Allows to exit cell execution in Jupyter\n",
    "class ExitCell(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "# # Import Sionna\n",
    "# try:\n",
    "#     import sionna\n",
    "# except ImportError as e:\n",
    "#     # Install Sionna if package is not already installed\n",
    "#     import os\n",
    "#     os.system(\"pip install sionna\")\n",
    "#     import sionna\n",
    "\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# Avoid warnings from TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tf.random.set_seed(1) # Set global random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import Sionna RT components\n",
    "from sionna.rt import load_scene, Transmitter, Receiver, PlanarArray, Camera, AssetObject\n",
    "\n",
    "# For link-level simulations\n",
    "from sionna.channel import cir_to_ofdm_channel, subcarrier_frequencies, OFDMChannel, ApplyOFDMChannel, CIRDataset\n",
    "from sionna.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver\n",
    "from sionna.utils import compute_ber, ebnodb2no, PlotBER\n",
    "from sionna.ofdm import KBestDetector, LinearDetector\n",
    "from sionna.mimo import StreamManagement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58380340",
   "metadata": {},
   "source": [
    "## Loading Scenes\n",
    "\n",
    "The Sionna RT module can either load external scene files (in Mitsuba's XML file format) or it can load one of the [integrated scenes](https://nvlabs.github.io/sionna/api/rt.html#example-scenes).\n",
    "\n",
    "In this example, we load an example scene containing one basic square room. The scene can be seen as a the static reference, on which one can add or remove asset objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66aa238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load scene\n",
    "scene = load_scene(filename=\"../Blender/base_room/base_room.xml\")\n",
    "\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512);\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec6563",
   "metadata": {},
   "source": [
    "## Loading Assets\n",
    "\n",
    "In a static scene one can add asset objects at various position to evaluate different scenario without having to design a dedicated scene each time in Blender. One could also perform this, as a workaround, by using mobility tools from Sionna R0.17 to move existing objects into or out of the zone of interest of the scene. Yet, this would requires to have put this object in the scene in the first place.\n",
    "\n",
    "Asset objects are added to the scene by recreating the scene from scratch, hence this process break any differentiablity properties. This tools must thus be seen as a way to programmatically construct the scene (as if working in Blender prior to the import in Sionna)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scene = load_scene(filename=\"../Blender/openspace/openspace.xml\")\n",
    "position = [np.random.randint(1,9),np.random.randint(-11,-1),0]\n",
    "orientation = [0,0,np.random.randint(0,360)]\n",
    "\n",
    "asset = AssetObject(f\"asset_0\", filename=\"./sionna/rt/assets/body.xml\",position=position, orientation=orientation)\n",
    "scene.add(asset)\n",
    "\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6bb8e",
   "metadata": {},
   "source": [
    "## Removing Assets\n",
    "\n",
    "Asset objects can be accessed using their unique id name. \n",
    "One can remove existing asset in the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d623a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.remove(\"asset_0\")\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a106df1",
   "metadata": {},
   "source": [
    "## Rendering images\n",
    "It is often convenient to choose a viewpoint in the 3D preview prior to rendering it as a high-quality image.\n",
    "The next cell uses the \"preview\" camera which corresponds to the viewpoint of the current preview image.\n",
    "Instead of the preview camera, one can also specify dedicated cameras with different positions and `look_at` directions.\n",
    "One can also render the image to a file as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fe2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Render in notebook\n",
    "\n",
    "# Add an asset to the scene\n",
    "position = [np.random.randint(1,9),np.random.randint(-11,-1),0]\n",
    "orientation = [0,0,np.random.randint(0,360)]\n",
    "\n",
    "asset = AssetObject(f\"asset_0\", filename=\"./sionna/rt/assets/body.xml\",position=position, orientation=orientation)\n",
    "scene.add(asset)\n",
    "\n",
    "# Create new camera with different configuration\n",
    "my_cam = Camera(\"my_cam\", position=[9,-11.5,2.1], look_at=[0,0,0])\n",
    "scene.remove(\"my_cam\")\n",
    "scene.add(my_cam)\n",
    "\n",
    "# Render scene with new camera*\n",
    "resolution = [1024,1024]\n",
    "num_samples=256 # Increase num_samples to increase image quality\n",
    "fov=110\n",
    "\n",
    "#Render to file\n",
    "render_to_file = False # Set to True to render image to file\n",
    "# Render scene to file from preview viewpoint\n",
    "if render_to_file:\n",
    "    scene.preview()\n",
    "    scene.render_to_file(camera=\"my_cam\", # Also try camera=\"preview\"\n",
    "                         filename=\"scene.png\",\n",
    "                         resolution=resolution,\n",
    "                         num_samples=num_samples,\n",
    "                         fov=fov)\n",
    "else:\n",
    "    scene.render(\"my_cam\", resolution=resolution, num_samples=num_samples,fov=fov); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7879019",
   "metadata": {},
   "source": [
    "## Ray Tracing for Radio Propagation\n",
    "\n",
    "We need to configure transmitters and receivers prior to computing propagation paths between them. All transmitters and all receivers are equipped with the same antenna arrays which are defined by the `scene` properties `scene.tx_array` and `scene.rx_array`, respectively. Antenna arrays are composed of multiple identical antennas. Antennas can have custom or pre-defined patterns and are either single- or dual-polarized. One can add multiple transmitters and receivers to a scene which need to have unique names, a position, and orientation which is defined by yaw, pitch, and roll angles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffabe8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure antenna array for all transmitters\n",
    "scene.tx_array = PlanarArray(num_rows=1,\n",
    "                             num_cols=2,\n",
    "                             vertical_spacing=0.5,\n",
    "                             horizontal_spacing=0.5,\n",
    "                             pattern=\"tr38901\",\n",
    "                             polarization=\"V\")\n",
    "\n",
    "# Configure antenna array for all receivers\n",
    "scene.rx_array = PlanarArray(num_rows=1,\n",
    "                             num_cols=2,\n",
    "                             vertical_spacing=0.5,\n",
    "                             horizontal_spacing=0.5,\n",
    "                             pattern=\"dipole\",\n",
    "                             polarization=\"cross\")\n",
    "\n",
    "# Create transmitter\n",
    "tx = Transmitter(name=\"tx\",\n",
    "                 position=[9,-11,2],\n",
    "                 look_at=[0,0,0])\n",
    "\n",
    "# Add transmitter instance to scene\n",
    "scene.remove(\"tx\")\n",
    "scene.add(tx)\n",
    "\n",
    "# Create a receiver\n",
    "rx = Receiver(name=\"rx\",\n",
    "              position=[1,-1,2],\n",
    "              look_at=[10,-12,0])\n",
    "\n",
    "# Add receiver instance to scene\n",
    "scene.remove(\"rx\")\n",
    "scene.add(rx)\n",
    "\n",
    "\n",
    "#tx.look_at(rx) # Transmitter points towards receiver\n",
    "\n",
    "scene.frequency = 2.14e9 # in Hz; implicitly updates RadioMaterials\n",
    "scene.synthetic_array = True # If set to False, ray tracing will be done per antenna element (slower for large arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1a5cf",
   "metadata": {},
   "source": [
    "Let us run the ray tracing process and compute propagation paths between all transmitters and receivers. The parameter `max_depth` determines the maximum number of interactions between a ray and a scene objects. \n",
    "For example, with a `max_depth` of one, only LoS paths are considered. When the property `scene.synthetic_array` is set to `True`, antenna arrays are explicitly modeled by finding paths between any pair of transmitting and receiving antennas in the scene. Otherwise, arrays are represented by a single antenna located in the center of the array.\n",
    "Phase shifts related to the relative antenna positions will then be applied based on a plane-wave assumption when the channel impulse responses are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa386d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute propagation paths\n",
    "paths = scene.compute_paths(max_depth=6,\n",
    "                            num_samples=1e4)  # Number of rays shot into directions defined\n",
    "                                              # by a Fibonacci sphere , too few rays can\n",
    "                                              # lead to missing paths\n",
    "\n",
    "# Visualize paths in the 3D preview\n",
    "if colab_compat:\n",
    "    scene.render(\"my_cam\", paths=paths, show_devices=True, show_paths=True, resolution=resolution);\n",
    "    raise ExitCell\n",
    "scene.preview(paths, show_devices=True, show_paths=True) # Use the mouse to focus on the visualized paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37212844",
   "metadata": {},
   "source": [
    "The [Paths](https://nvlabs.github.io/sionna/api/rt.html#paths) object contains all paths that have been found between transmitters and receivers.\n",
    "In principle, the existence of each path is determininistic for a given position and environment. Please note that due to the stochastic nature of the *shoot-and-bounce* algorithm, different runs of the `compute_paths` function can lead to different paths that are found. Most importantly, diffusely reflected or scattered paths are obtained through random sampling of directions after each interaction with a scene object. You can seet TensorFlow's random seed to a specific value before executing ``compute_paths`` to ensure reproducibility.\n",
    "\n",
    "The Paths object contains detailed information about every found path and allows us to generated channel impulse responses and apply Doppler shifts for the simulation of time evolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a6a71",
   "metadata": {},
   "source": [
    "## From Paths to Channel Impulse Responses\n",
    "\n",
    "Once paths are computed, they can be transformed into channel impulse responses (CIRs).\n",
    "The class method [apply_doppler](https://nvlabs.github.io/sionna/api/rt.html#Paths.apply_doppler) can simulate time evolution of the CIR based on arbitrary velocity vectors of all transmitters and receivers for a desired sampling frequency and number of time steps. \n",
    "The class method [cir](https://nvlabs.github.io/sionna/api/rt.html#Paths.cir) generates the channel impulse responses which can be used by other components for link-level simulations in either time or frequency domains. The method also allows you to only consider certain types of paths, e.g., line-of-sight, reflections, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21043de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default parameters in the PUSCHConfig\n",
    "subcarrier_spacing = 15e3\n",
    "fft_size = 48\n",
    "\n",
    "# Print shape of channel coefficients before the application of Doppler shifts\n",
    "# The last dimension corresponds to the number of time steps which defaults to one\n",
    "# as there is no mobility\n",
    "print(\"Shape of `a` before applying Doppler shifts: \", paths.a.shape)\n",
    "\n",
    "# Apply Doppler shifts (here no doppler since TX/RX are static)\n",
    "paths.apply_doppler(sampling_frequency=subcarrier_spacing, # Set to 15e3 Hz\n",
    "                    num_time_steps=20, # Number of OFDM symbols\n",
    "                    tx_velocities=[0,0,0], # We can set additional tx speeds\n",
    "                    rx_velocities=[0,0,0]) # Or rx speeds\n",
    "\n",
    "print(\"Shape of `a` after applying Doppler shifts: \", paths.a.shape)\n",
    "\n",
    "a, tau = paths.cir()\n",
    "print(\"Shape of tau: \", tau.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c4ce8",
   "metadata": {},
   "source": [
    "Let us have a look at the channel impulse response for the 14 incoming paths from the simulation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717a4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = tau[0,0,0,:]/1e-9 # Scale to ns\n",
    "a_abs = np.abs(a)[0,0,0,0,0,:,0]\n",
    "a_max = np.max(a_abs)\n",
    "# Add dummy entry at start/end for nicer figure\n",
    "t = np.concatenate([(0.,), t, (np.max(t)*1.1,)])\n",
    "a_abs = np.concatenate([(np.nan,), a_abs, (np.nan,)])\n",
    "\n",
    "# And plot the CIR\n",
    "plt.figure()\n",
    "plt.title(\"Channel impulse response realization\")\n",
    "\n",
    "plt.stem(t, a_abs)\n",
    "plt.xlim([0, np.max(t)])\n",
    "plt.ylim([-2e-6, a_max*1.1])\n",
    "plt.xlabel(r\"$\\tau$ [ns]\")\n",
    "plt.ylabel(r\"$|a|$\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b133a1",
   "metadata": {},
   "source": [
    "The CIRs can now be loaded either in the time-domain or frequency-domain channel models, respectively.\n",
    "Please see [cir_to_ofdm_channel](https://nvlabs.github.io/sionna/api/channel.wireless.html#sionna.channel.cir_to_ofdm_channel) and [cir_to_time_channel](https://nvlabs.github.io/sionna/api/channel.wireless.html#sionna.channel.cir_to_time_channel) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c0cad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute frequencies of subcarriers and center around carrier frequency\n",
    "frequencies = subcarrier_frequencies(fft_size, subcarrier_spacing)\n",
    "\n",
    "# Compute the frequency response of the channel at frequencies.\n",
    "h_freq = cir_to_ofdm_channel(frequencies,\n",
    "                             a,\n",
    "                             tau,\n",
    "                             normalize=True) # Non-normalized includes path-loss\n",
    "\n",
    "# Verify that the channel power is normalized\n",
    "h_avg_power = tf.reduce_mean(tf.abs(h_freq)**2).numpy()\n",
    "\n",
    "print(\"Shape of h_freq: \", h_freq.shape)\n",
    "print(\"Average power h_freq: \", h_avg_power) # Channel is normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535060e5",
   "metadata": {},
   "source": [
    "The frequency responses `h_freq` are now ready to be processed by the [ApplyOFDMChannel](https://nvlabs.github.io/sionna/api/channel.wireless.html#sionna.channel.ApplyOFDMChannel) Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63824e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Placeholder for tx signal of shape\n",
    "# [batch size, num_tx, num_tx_ant, num_ofdm_symbols, fft_size]\n",
    "x = tf.zeros([h_freq.shape.as_list()[i] for i in [0,3,4,5,6]], tf.complex64)\n",
    "\n",
    "no = 0.1 # noise variance\n",
    "\n",
    "# Init channel layer\n",
    "channel = ApplyOFDMChannel(add_awgn=True)\n",
    "\n",
    "# Apply channel\n",
    "y = channel([x, h_freq, no])\n",
    "\n",
    "# [batch size, num_rx, num_rx_ant, num_ofdm_symbols, fft_size]\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a61fb",
   "metadata": {},
   "source": [
    "## Coverage Map\n",
    "Sionna RT can be used to simulate coverage maps for a given environment. Let's see the effect of multiple asset objects on the transmitted beams of a directive antenna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c3bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add multiples assets to the scene: \n",
    "for i in range(40):\n",
    "    # Add asset i to the scene\n",
    "    position = [np.random.randint(1,9),np.random.randint(-11,-1),0]\n",
    "    orientation = [0,0,np.random.randint(0,360)]\n",
    "\n",
    "    asset = AssetObject(f\"asset_{i}\", filename=\"./sionna/rt/assets/body.xml\",position=position, orientation=orientation)\n",
    "    scene.add(asset)\n",
    "\n",
    "# Remove old transmitter and add new one\n",
    "scene.tx_array = PlanarArray(num_rows=1,\n",
    "                             num_cols=16,\n",
    "                             vertical_spacing=0.5,\n",
    "                             horizontal_spacing=0.5,\n",
    "                             pattern=\"tr38901\",\n",
    "                             polarization=\"V\")\n",
    "\n",
    "tx = Transmitter(name=\"tx\",\n",
    "                 position=[9,-11,1.8], \n",
    "                 look_at=[0,0,0])\n",
    "scene.remove(\"tx\")\n",
    "scene.add(tx)\n",
    "\n",
    "\n",
    "# We could have alternatively modified the properties position and orientation of the existing transmitter\n",
    "#scene.get(\"tx\").position = [-210,73,105]\n",
    "#scene.get(\"tx\").orientation = [0,0,0]\n",
    "\n",
    "cm = scene.coverage_map(max_depth=8,\n",
    "                        diffraction=True, # Disable to see the effects of diffraction\n",
    "                        cm_cell_size=(.1, .1), # Grid size of coverage map cells in m\n",
    "                        combining_vec=None,\n",
    "                        precoding_vec=None,\n",
    "                        num_samples=int(2e6)) # Reduce if your hardware does not have enough memory\n",
    "                        \n",
    "# Create new camera\n",
    "tx_pos = scene.transmitters[\"tx\"].position.numpy()\n",
    "bird_pos = tx_pos.copy()\n",
    "bird_pos[-1] = 1000 # Set height of coverage map to 1000m above tx\n",
    "bird_pos[-2]-= 0.01 # Slightly move the camera for correct orientation\n",
    "\n",
    "# Create new camera\n",
    "bird_cam = Camera(\"birds_view\", position=bird_pos, look_at=tx_pos)\n",
    "\n",
    "scene.remove(\"birds_view\")\n",
    "scene.add(bird_cam)\n",
    "\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"birds_view\", coverage_map=cm, num_samples=512, resolution=resolution);\n",
    "    raise ExitCell\n",
    "# Open 3D preview (only works in Jupyter notebook)\n",
    "scene.preview(coverage_map=cm)\n",
    "# cm.show(tx=0); # If multiple transmitters exist, tx selects for which transmitter the cm is shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba012f88",
   "metadata": {},
   "source": [
    "Note that it can happen in rare cases that diffracted rays arrive inside or behind buildings through paths which should not exists. This is not a bug in Sionna's ray tracing algorithm but rather an artefact of the way how scenes are created which can lead to the false detection of diffraction edges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80096243",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion and Outlook\n",
    "\n",
    "In this notebook, you have learned how to add assets to a scene within Sionna."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
