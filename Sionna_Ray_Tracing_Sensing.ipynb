{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e6b8cc",
   "metadata": {},
   "source": [
    "# To Add\n",
    "\n",
    "- base room .xml à ajouter au scene de Sionna\n",
    "- accéder aux SceneObject depuis les AssetObject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a8731",
   "metadata": {},
   "source": [
    "# Introduction to Sionna RT - Scene and Assets Configuration Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702817e",
   "metadata": {},
   "source": [
    "In this notebook, you will\n",
    "- Learn how to programaticaly append or remove assets object to a scene (without using Blender).\n",
    "- See the impact of the assets within the scene w.r.t. ray-traced channels for link-level simulations instead of stochastic channel models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0a988",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Information On Assets](#Information-On-Assets)\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32f3d7",
   "metadata": {},
   "source": [
    "## Information On Assets\n",
    "\n",
    "It is usefull to be able to construct a scene dynamically by adding and removing objects, referred to as assets, thus being able to automatically generate datasets from the scenes and/or define complex scenario. To this end, we propose a few novel functionalities. One can define a scene, e.g. using Blender, import the scene within Sionna and then add new assets to that scene. The assets can also be separately defined in Blender for instance. As an example, one could define a scene with a car park, and append to that scene a varing number of cars to see the impact in terms of RF propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938106f2",
   "metadata": {},
   "source": [
    "## GPU Configuration and Imports <a class=\"anchor\" id=\"GPU-Configuration-and-Imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c23abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_num = 0 # Use \"\" to use the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Colab does currently not support the latest version of ipython.\n",
    "# Thus, the preview does not work in Colab. However, whenever possible we\n",
    "# strongly recommend to use the scene preview mode.\n",
    "try: # detect if the notebook runs in Colab\n",
    "    import google.colab\n",
    "    colab_compat = True # deactivate preview\n",
    "except:\n",
    "    colab_compat = False\n",
    "resolution = [480,320] # increase for higher quality of renderings\n",
    "\n",
    "# Allows to exit cell execution in Jupyter\n",
    "class ExitCell(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# Avoid warnings from TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tf.random.set_seed(1) # Set global random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b7d49",
   "metadata": {},
   "source": [
    "# To Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Import Sionna RT components\n",
    "from sionna.rt import load_scene, Transmitter, Receiver, PlanarArray, Camera, AssetObject\n",
    "\n",
    "# For link-level simulations\n",
    "from sionna.channel import cir_to_time_channel, subcarrier_frequencies, OFDMChannel, ApplyOFDMChannel, CIRDataset\n",
    "from sionna.nr import PUSCHConfig, PUSCHTransmitter, PUSCHReceiver\n",
    "from sionna.utils import compute_ber, ebnodb2no, PlotBER\n",
    "from sionna.ofdm import KBestDetector, LinearDetector\n",
    "from sionna.mimo import StreamManagement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58380340",
   "metadata": {},
   "source": [
    "## Loading Scenes\n",
    "\n",
    "The Sionna RT module can either load external scene files (in Mitsuba's XML file format) or it can load one of the [integrated scenes](https://nvlabs.github.io/sionna/api/rt.html#example-scenes).\n",
    "\n",
    "In this example, we load a scene containing a floor and a basic wall above it. The scene can be seen as a the static reference, on which one can add or remove assets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66aa238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load scene\n",
    "scene = load_scene(filename=\"./sionna/rt/scenes/floor_wall/floor_wall.xml\")\n",
    "\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512);\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec6563",
   "metadata": {},
   "source": [
    "## Load and Handle Assets\n",
    "\n",
    "In a static scene, one can add assets at various position to evaluate different scenario without having to design a dedicated scene beforehand. It should be noted, that assets are added to the scene by recreating the scene from scratch. Hence, the action to add or remove an asset object to a scene is not differentiable, but the previous and the novel scenes preserve their differentiability properties as any scene in Sionna. This tool must be seen as a way to programmatically construct the scene, e.g. to generate many random scenes to construct a dataset for neural networks training.\n",
    "\n",
    "Some assets are located in ./sionna/rt/assets. Let's see how to load and use them.\n",
    "\n",
    "NB: As a workaround to maintain differentiability when placing an object, one could use the mobility tools from Sionna to move pre-existing objects of the scene into or out of the zone of interest of the scene. Yet, this would require to have put these objects in the scene in the first place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48401ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an asset\n",
    "asset = AssetObject(name=\"asset_0\", filename=\"./sionna/rt/assets/body/body.xml\")\n",
    "\n",
    "# Add the asset to the scene\n",
    "scene.add(asset)\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66efde2",
   "metadata": {},
   "source": [
    "As it can be seen in the preview, a body has been added to the previous scene, by creating a novel scene from scratch.\n",
    "\n",
    "The body can be handle in the same way as any object of the scene.\n",
    "\n",
    "It is listed in the .asset_objects method from the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the assets dictionnary from the scene\n",
    "print(scene.asset_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed615dbd",
   "metadata": {},
   "source": [
    "The dictionnary contains all the assets from the scene.\n",
    "\n",
    "Our body \"asset_0\" is listed as an AssetObject. They are slithly different from others Sionna's SceneObject, as we will demonstrate in this notebook. \n",
    "\n",
    "First of all, AssetObject can be composed of several SceneObject. Use the method .shapes to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the asset object by name as any object from the scene\n",
    "body_asset = scene.get('asset_0')\n",
    "\n",
    "# Print the objects (SceneObject) composing the asset (AssetObject)\n",
    "print(f\"{body_asset.shapes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c024452",
   "metadata": {},
   "source": [
    "Our asset (AssetObject) \"asset_0\" is composed of one object (SceneObject) \"asset_0-body\".\n",
    "\n",
    "The name of the object is based on the asset name \"asset_0\" associated with the component name defined in Blender \"body\".\n",
    "\n",
    "The SceneObject can be seen in the object dictionnary of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a dictionnary containing all the objects of the scene\n",
    "print(scene.objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0454e3",
   "metadata": {},
   "source": [
    "The scene contains three objects:\n",
    "- The \"floor\" that was in the basic scene.\n",
    "- The \"wall\" that was also in the basic scene.\n",
    "- The \"body\", named \"asset_0-body\", that we have added.\n",
    "\n",
    "Hence, the body is seen as a classic SceneObject by Sionna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the body object from the scene as any SceneObject\n",
    "body_object = scene.get('asset_0-body')\n",
    "\n",
    "\n",
    "print(f\"Current body asset position: {body_asset.position.numpy()}\")\n",
    "print(f\"Current body asset orientation: {body_asset.orientation.numpy()}\")\n",
    "# print(f\"Current body asset velocity: {body_asset.velocity.numpy()}\")\n",
    "\n",
    "print(f\"Current body object position: {body_object.position.numpy()}\")\n",
    "print(f\"Current body object orientation: {body_object.orientation.numpy()}\")\n",
    "print(f\"Current body object velocity: {body_object.velocity.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350a446",
   "metadata": {},
   "source": [
    "As it can be seen, the position of the object is not [0.,0.,0.], contrary to the asset one. This is because Sionna creates a virtual hit box around the whole asset and return the position of the barycenter of this box. Hence, this position is somewhere at the center of the body. The asset barycenter is the one defined originally in Blender, at the feet. To witness that behavior, we can simply fix the position of the object to [0.,0.,0.] using the SceneObject property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_body_object_position = body_object.position.numpy()\n",
    "body_object.position = [0.,0.,0.]\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f176595",
   "metadata": {},
   "source": [
    "The barycenter of our asset is now at the coordinates [0.,0.,0.]. But this behavior doesn't really assist the user in configuring the scene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f72725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the orientation of the SceneObject\n",
    "body_object.orientation = [np.pi/4.,np.pi/4.,np.pi/4.]\n",
    "\n",
    "print(f\"Orientation of the object: {body_object.orientation.numpy()}\")\n",
    "print(f\"Position of the object: {body_object.position.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089ce69",
   "metadata": {},
   "source": [
    "After the change of orientation, the position of the barycenter of the object has change. That is because Sionna's hit box is not rotated, but recomputed with edges parallel to the coordinate reference axis (x,y,z), hence modifying the barycenter position.\n",
    "\n",
    "That's why we advice to directly manipulate the asset itself, as you will be able to conveniently and consistently manipulate group of objects together. Also, this is more natural, as the origin of the asset is defined by the user through the asset generation in Blender.\n",
    "\n",
    "That's why modifying the position/orientation/velocity of an object from an asset will not modify the asset's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that the position and orientation of the asset has not been modified\n",
    "print(f\"Position of the asset: {body_asset.position.numpy()}\")\n",
    "print(f\"Orientation of the asset: {body_asset.orientation.numpy()}\")\n",
    "\n",
    "# Position the body at its original position, with original orientation\n",
    "# First set the orientation and then the position to avoid side effect\n",
    "body_object.orientation = [0.,0.,0.]\n",
    "body_object.position = original_body_object_position\n",
    "\n",
    "print(f\"Position of the object: {body_object.position.numpy()}\")\n",
    "print(f\"Orientation of the object: {body_object.orientation.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439316de",
   "metadata": {},
   "source": [
    "The body is back on its feet! Now let's move to one side of the wall using the asset properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the body to one side of the wall\n",
    "body_asset.position = [1.,0.,0.]\n",
    "\n",
    "print(f\"Position of the asset: {body_asset.position.numpy()}\")\n",
    "print(f\"Orientation of the asset: {body_asset.orientation.numpy()}\")\n",
    "\n",
    "print(f\"Position of the object: {body_object.position.numpy()}\")\n",
    "print(f\"Orientation of the object: {body_object.orientation.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d0a92",
   "metadata": {},
   "source": [
    "The body object position has been correctly moved along with the asset one. Now let's change its orientation so it faces the wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the body face the wall\n",
    "body_asset.orientation = [-np.pi/2.,0.,0.]\n",
    "\n",
    "print(f\"Position of the asset: {body_asset.position.numpy()}\")\n",
    "print(f\"Orientation of the asset: {body_asset.orientation.numpy()}\")\n",
    "\n",
    "print(f\"Position of the object: {body_object.position.numpy()}\")\n",
    "print(f\"Orientation of the object: {body_object.orientation.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834379f7",
   "metadata": {},
   "source": [
    "The center of the body asset is located at its feet. So we can turn it upside down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91927d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the body upside down\n",
    "body_asset.orientation += [0.,np.pi,0.]\n",
    "\n",
    "print(f\"Position of the asset: {body_asset.position.numpy()}\")\n",
    "print(f\"Orientation of the asset: {body_asset.orientation.numpy()}\")\n",
    "\n",
    "print(f\"Position of the object: {body_object.position.numpy()}\")\n",
    "print(f\"Orientation of the object: {body_object.orientation.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba238f",
   "metadata": {},
   "source": [
    "So now that we have play a little with a single body, we will remove it from the scene and use a two bodies asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the body asset\n",
    "scene.remove('asset_0')\n",
    "\n",
    "print(f\"Scene assets: {scene.asset_objects}\")\n",
    "print(f\"Scene objects: {scene.objects}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27179342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an asset\n",
    "asset = AssetObject(name=\"asset_0\", filename=\"./sionna/rt/assets/two_persons/two_persons.xml\")\n",
    "\n",
    "# Add the asset to the scene\n",
    "scene.add(asset)\n",
    "\n",
    "print(f\"Scene assets: {scene.asset_objects}\")\n",
    "print(f\"Scene objects: {scene.objects}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e8a9f",
   "metadata": {},
   "source": [
    "Our asset is composed of two objects \"asset_0-person_1\" and \"asset_0-person_2\", as it can be seen in the objects list of the scene.\n",
    "\n",
    "We can use the asset properties to move the bodies around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get assets and objects\n",
    "bodies_asset = asset\n",
    "body_1_object = scene.get('asset_0-person_1')\n",
    "body_2_object = scene.get('asset_0-person_2')\n",
    "\n",
    "# Move the two bodies to one side of the wall\n",
    "bodies_asset.position = [1.,0.,0.]\n",
    "\n",
    "print(f\"Position of the asset: {bodies_asset.position.numpy()}\")\n",
    "print(f\"Orientation of the asset: {bodies_asset.orientation.numpy()}\")\n",
    "\n",
    "print(f\"Position of the person 1 object: {body_1_object.position.numpy()}\")\n",
    "print(f\"Orientation of the person 1 object: {body_1_object.orientation.numpy()}\") \n",
    "\n",
    "print(f\"Position of the person 2 object: {body_2_object.position.numpy()}\")\n",
    "print(f\"Orientation of the person 2 object: {body_2_object.orientation.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a56ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the two bodies to each side of the wall\n",
    "bodies_asset.position = [0.,0.,0.]\n",
    "bodies_asset.orientation = [np.pi/2.,0.,0.]\n",
    "\n",
    "print(f\"Position of the asset: {bodies_asset.position.numpy()}\")\n",
    "print(f\"Orientation of the asset: {bodies_asset.orientation.numpy()}\")\n",
    "\n",
    "print(f\"Position of the person 1 object: {body_1_object.position.numpy()}\")\n",
    "print(f\"Orientation of the person 1 object: {body_1_object.orientation.numpy()}\") \n",
    "\n",
    "print(f\"Position of the person 2 object: {body_2_object.position.numpy()}\")\n",
    "print(f\"Orientation of the person 2 object: {body_2_object.orientation.numpy()}\")\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c324d2cb",
   "metadata": {},
   "source": [
    "Now we will demonstrate that the assets have the same interaction with ray tracing than any other Sionna SceneObject.\n",
    "\n",
    "We will add a TX/RX couple at each side of the wall.\n",
    "\n",
    "The bodies objects will be individually moved (again this is not recommanded), so that we increase the space between them and can use them as reflectors.\n",
    "\n",
    "With only reflection activate, we should be able to see 3 paths: one reflected on the floor and two reflected by the bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the bodies\n",
    "bodies_asset.orientation = [0.,0.,0.]\n",
    "body_1_object.position += [0.,1.5,0.]\n",
    "body_2_object.position += [0.,-1.5,0.]\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the transmitter and receiver arrays\n",
    "scene.tx_array = PlanarArray(num_rows=1,\n",
    "                             num_cols=1,\n",
    "                             vertical_spacing=0.5,\n",
    "                             horizontal_spacing=0.5,\n",
    "                             pattern=\"iso\",\n",
    "                             polarization=\"V\")\n",
    "\n",
    "scene.rx_array = scene.tx_array\n",
    "\n",
    "# Add a transmitter and receiver with equal distance from the center of the surface\n",
    "# The position is set precisely to get perfect reflexion from the bodies hands\n",
    "scene.add(Transmitter(name=\"tx\", position=[-3.1,0.,2.1]))\n",
    "scene.add(Receiver(name=\"rx\", position=[3.1,0.,2.1]))\n",
    "\n",
    "# Preview the scene\n",
    "if colab_compat:\n",
    "    scene.render(camera=\"scene-cam-0\", num_samples=512)\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the scene frequency\n",
    "scene.frequency = 2.4e9\n",
    "\n",
    "# No LOS, one reflection max\n",
    "paths = scene.compute_paths(los=False, reflection=True, max_depth=1, num_samples=1e6, method='exhaustive')\n",
    "\n",
    "print(np.abs(paths.a[0,0,0,0,0,:,0].numpy()))\n",
    "print(paths.tau[0,0,0,:].numpy())\n",
    "\n",
    "# Open 3D preview (only works in Jupyter notebook)\n",
    "if colab_compat:\n",
    "    scene.render(\"scene-cam-0\", paths=paths);\n",
    "    raise ExitCell\n",
    "scene.preview(paths=paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8354ca2",
   "metadata": {},
   "source": [
    "The bodies provided in the asset are composed of numerous small reflective surfaces. The positions of TX and RX were chosen to obtain reflections on the hands.\n",
    "\n",
    "NB: To see more paths, one could activate the scattering.\n",
    "\n",
    "There are exactly 9 paths in the scene:\n",
    "* The first path in time is reflected on the floor.\n",
    "* The 8 other paths are reflected on the hands. You can zoom on the preview to see the 8 paths. Because of the symetry of the scene, these paths arrive at (almost) the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, tau = paths.cir()\n",
    "t = tau[0,0,0,:]/1e-9 # Scale to ns\n",
    "a_abs = np.abs(a)[0,0,0,0,0,:,0]\n",
    "a_max = np.max(a_abs)\n",
    "\n",
    "# Add dummy entry at start/end for nicer figure\n",
    "t = np.concatenate([(0.,), t, (np.max(t)*1.1,)])\n",
    "a_abs = np.concatenate([(np.nan,), a_abs, (np.nan,)])\n",
    "\n",
    "# And plot the CIR\n",
    "plt.figure()\n",
    "plt.title(\"Channel impulse response realization\")\n",
    "\n",
    "# plt.stem(t, a_abs)\n",
    "plt.stem(t, a_abs)\n",
    "plt.xlim([0, np.max(t)])\n",
    "plt.ylim([-2e-6, a_max*1.1])\n",
    "plt.xlabel(r\"$\\tau$ [ns]\")\n",
    "plt.ylabel(r\"$|a|$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fca0ec",
   "metadata": {},
   "source": [
    "On the CIR, we can see the 8 paths reflected by the hands arrived at the same time, around 2.45 ns. Generating the time channel estimate from the CIR should merge the power of these paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eaeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency response of the channel at frequencies.\n",
    "sampling_freq = 5e9 # 0.2 ns resolution\n",
    "l_min = -6\n",
    "l_max = 20\n",
    "h_time = cir_to_time_channel(bandwidth=sampling_freq, a=a, tau=tau, l_min=l_min, l_max=l_max, normalize=True)\n",
    "t_time = np.arange(l_min, (l_max+1), 1) * (1/sampling_freq)\n",
    "\n",
    "h_time_abs = np.abs(h_time[0,0,0,0,0,0,:].numpy())\n",
    "\n",
    "# And plot the CIR\n",
    "plt.figure()\n",
    "plt.title(\"Time Channel from CIR\")\n",
    "plt.stem(t_time, h_time_abs)\n",
    "plt.xlim([0, np.max(t_time)])\n",
    "# plt.ylim([-2e-6, a_max*1.1])\n",
    "plt.xlabel(r\"$\\tau$ [ns]\")\n",
    "plt.ylabel(r\"$|h_time|$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a0d31",
   "metadata": {},
   "source": [
    "Because of the 8 paths reflected by the hands, most of the energy arrive through these paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e94a4",
   "metadata": {},
   "source": [
    "# Material \n",
    "\n",
    "# BSDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f50fc0",
   "metadata": {},
   "source": [
    "We can modify the materials of individual objects composing an asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(scene.radio_materials.keys()))\n",
    "\n",
    "# Set the material of the first body to brick\n",
    "body_1_object.radio_material = \"itu_brick\"\n",
    "\n",
    "print(f\"First body material: {body_1_object.radio_material.name}\")\n",
    "print(f\"Second body material: {body_2_object.radio_material.name}\")\n",
    "\n",
    "# Open 3D preview (only works in Jupyter notebook)\n",
    "if colab_compat:\n",
    "    scene.render(\"scene-cam-0\", paths=paths);\n",
    "    raise ExitCell\n",
    "scene.preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b427258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No LOS, one reflection max\n",
    "paths = scene.compute_paths(los=False, reflection=True, max_depth=1, num_samples=1e6, method='exhaustive')\n",
    "\n",
    "print(np.abs(paths.a[0,0,0,0,0,:,0].numpy()))\n",
    "\n",
    "# Open 3D preview (only works in Jupyter notebook)\n",
    "if colab_compat:\n",
    "    scene.render(\"scene-cam-0\", paths=paths);\n",
    "    raise ExitCell\n",
    "scene.preview(paths=paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393fb882",
   "metadata": {},
   "source": [
    "From the paths absolute values, we can separate the paths that reflected on the body in concrete, and the one on the body in brick."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80096243",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion and Outlook"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
